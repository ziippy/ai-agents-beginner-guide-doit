{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유투브 영상 요약하기\n",
    "\n",
    "유투브에서 원하는 내용을 검색하고 텍스트로 가져와 언어 모델로 응용하는 방법을 다뤄보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-search\n",
      "  Using cached youtube_search-2.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: requests in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from youtube-search) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube-search) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube-search) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube-search) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube-search) (2025.8.3)\n",
      "Using cached youtube_search-2.1.2-py3-none-any.whl (3.4 kB)\n",
      "Installing collected packages: youtube-search\n",
      "Successfully installed youtube-search-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Sl9EZpE61xA',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAI2p1dU9p9q97m8Tj2tyPewluofA',\n",
       "   'https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBpcJB5QG7qsRvGD7LAvzcWB_uUlg'],\n",
       "  'title': 'What is A2A (Agent to Agent Protocol)? | A2A Explained',\n",
       "  'long_desc': None,\n",
       "  'channel': 'codebasics',\n",
       "  'duration': '13:03',\n",
       "  'views': '조회수 33,009회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=Sl9EZpE61xA&pp=ygUMV2hhdCBpcyBBMkE_'},\n",
       " {'id': 'WWHlehkRp3w',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDVSt8bALIgRKupS1h4d63Zr2dRxg',\n",
       "   'https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDkdltB-XhkieSR0Oq1XmLoCRqrLA'],\n",
       "  'title': \"Google's A2A Protocol in 100 Seconds (AI Agents)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Better Stack',\n",
       "  'duration': '2:26',\n",
       "  'views': '조회수 9,770회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=WWHlehkRp3w&pp=ygUMV2hhdCBpcyBBMkE_'},\n",
       " {'id': 'voaKr_JHvF4',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCcsxXPAhNn02WEnESMsRvhnXoglQ',\n",
       "   'https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCLHBbrCZWZhg1kQuDZV02U9lv2Ow'],\n",
       "  'title': \"What Makes Google's A2A Protocol REALLY POWERFUL (in 12 Minutes)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Jack Herrington',\n",
       "  'duration': '12:30',\n",
       "  'views': '조회수 55,512회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=voaKr_JHvF4&pp=ygUMV2hhdCBpcyBBMkE_'},\n",
       " {'id': 'GUozMSpnmcc',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAainynDDWkCS5D1lD9J_wiq90GfA',\n",
       "   'https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCL6gT6oBOergoFUfhxP4D1Rp7hKQ'],\n",
       "  'title': 'Google A2A Protocol Explained: Everything You Need to Know',\n",
       "  'long_desc': None,\n",
       "  'channel': 'AI LABS',\n",
       "  'duration': '8:20',\n",
       "  'views': '조회수 5,594회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'},\n",
       " {'id': 'IMcDEvXRBkY',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDQ5VbTviH_q8QXcX012o6KOSKl3w',\n",
       "   'https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCoS4vMaCArJgyD0UyAn-EEHhGJ4A'],\n",
       "  'title': 'MCP or A2A',\n",
       "  'long_desc': None,\n",
       "  'channel': \"John Savill's Technical Training\",\n",
       "  'duration': '19:59',\n",
       "  'views': '조회수 15,017회',\n",
       "  'publish_time': '1개월 전',\n",
       "  'url_suffix': '/watch?v=IMcDEvXRBkY&pp=ygUMV2hhdCBpcyBBMkE_'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 키워드 검색하기\n",
    "from youtube_search import YoutubeSearch\n",
    "\n",
    "videos = YoutubeSearch(\"What is A2A?\", max_results=5).to_dict()\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 온전한 경로 만들기\n",
    "video_url = 'https://youtube.com' + videos[3]['url_suffix']\n",
    "video_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YoutubeLoader 패키지로 유투브 자막 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube_transcript_api\n",
      "  Downloading youtube_transcript_api-1.2.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from youtube_transcript_api)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: requests in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from youtube_transcript_api) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube_transcript_api) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube_transcript_api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube_transcript_api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/haiqv/conda/envs/agent-llm/lib/python3.11/site-packages (from requests->youtube_transcript_api) (2025.8.3)\n",
      "Downloading youtube_transcript_api-1.2.2-py3-none-any.whl (485 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: defusedxml, youtube_transcript_api\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [youtube_transcript_api]\n",
      "\u001b[1A\u001b[2KSuccessfully installed defusedxml-0.7.1 youtube_transcript_api-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this video (voaKr_JHvF4) transcripts are available in the following languages:\n",
      "\n",
      "(MANUALLY CREATED)\n",
      "None\n",
      "\n",
      "(GENERATED)\n",
      " - en (\"English (auto-generated)\")[TRANSLATABLE]\n",
      "\n",
      "(TRANSLATION LANGUAGES)\n",
      " - ar (\"Arabic\")\n",
      " - zh-Hant (\"Chinese (Traditional)\")\n",
      " - nl (\"Dutch\")\n",
      " - fr (\"French\")\n",
      " - de (\"German\")\n",
      " - hi (\"Hindi\")\n",
      " - id (\"Indonesian\")\n",
      " - it (\"Italian\")\n",
      " - ja (\"Japanese\")\n",
      " - ko (\"Korean\")\n",
      " - pt (\"Portuguese\")\n",
      " - ru (\"Russian\")\n",
      " - es (\"Spanish\")\n",
      " - th (\"Thai\")\n",
      " - uk (\"Ukrainian\")\n",
      " - vi (\"Vietnamese\")\n",
      "[FetchedTranscriptSnippet(text='The AI world is back with another', start=0.08, duration=3.599), FetchedTranscriptSnippet(text='three-letter acronym. Last time it was', start=1.68, duration=4.639)]\n",
      "page_content='The AI world is back with another three-letter acronym. Last time it was MCP or the model context protocol. This time it's A2A from Google and it's the agent to agent protocol. Let's see what it is, how it works, and does it replace or complement MCP. Let's get right into [Music] it. that understand this ADA protocol. It's really good to understand MCP which is a related protocol. I've got a full overview video of that and there's a link in the description right down below. Now before we begin, one little thing. This is a copyrighted video and if you're not watching it on the blue color coder channel, you're not in the right place and I do not consent to this being rebroadcast on any other platform. So let's say that you want to book a flight. So you've got Claude desktop over here. Claude Desktop is an MCP client. It can connect to an MCP server from the airline and say, \"List tools. What do you have available for me to interact with your airline?\" The airline would send back, \"I've got tools to search flights and book flights.\" Your MCB client says, \"Hey, that's great. I want you to search for a flight for Portland to Honolulu.\" It sends back some JSON data encoded as text. Claude then selects the flight that I want to go on to and then invokes the bookflight tool which returns, hey, I got a flight for you. What's really going on here is that Claude is interacting with an LLM or a language learning model and invoking tools within that MCP server to get that done. So that's the model contracts protocol and people nowadays are calling it the USBC for AIS. A toa is this agentto agent protocol. So let's start talking about what an agent is. In the ADA protocol model, an agent is a combination of an LLM or large language model and tools. The agent takes an incoming task, uses the LLM to reason about that task, and then tools to execute on that task. And the ADA protocol allows either you or an agent to talk to another agent. So let's take that example scenario of you booking a flight. Now, you would have access to some AA client. Now that's actually not doable today. There are no kind of consumer A to A clients. All we have right now for A to A is this Google GitHub repo that has some sample code in it. It is not actually productized in any way. So this is what's going to be in the future. So you've got your A to client. It has access to an LLM. It sends off a chat style message. Please book a flight. Here are the specifics of that flight. That agent that exists within that ATA server then uses its own LLM to parse that request and use the tools that we talked about before searching flights and booking flights internally to go and book that flight. And then it sends back a chat style response. Again, agents are LLMs plus tools. And this is agents talking with other agents across that A2A protocol. Now, if that wasn't cool enough, the really interesting part about ADA is that I can connect with, for example, a travel agent agent. That travel agent would have its own LLM that it's connected to, but it would also be connected to downstream agents like an airline agent, a hotel agent, and a car agent. So conceivably I could make a request like hey go and book an entire trip to Honolulu and it would figure out I need to go on this airline at this time go into this hotel rent this car and all that and the way that it's going to do that is that that travel agent is going to use that same AA protocol to connect itself to the airline agent hotel agent and that car agent. So here's the ADA site. It's a really good read and you can also get some samples and in this case that is the Google ADA repo. Now that repo not only has example agents in both JavaScript and Python but it's also got a demo application that you can connect to that acts as a multi- aent kind of like the travel agent in our example. It's a single site that then connects to multiple agents simultaneously to try and get something done. So, I've actually built on top of that demo code and built a hotel agent and a flight agent that we can connect to on that demo. So, those are both running in my terminal. Here we have our hotel agent and here we have our flight agent and we've got the demo app running over here in our third terminal. So, let's go into our browser, see how we go. This is really beta stuff, so this might work and might not. So, this is the demo UI and it's actually currently hooked up to both those agents. So, let's try it out. Let's ask it to book a trip to Maui. All right, this is actually a pretty good response from what I've seen. So, it's telling us it's going to coordinate with both the flight agent and hotel agent. I apologize in advance for the CSS here. That's was part of the demo. All right, I'll give it a little more information and we'll see what happens. All right, it looks like it's interacting with the flight agent pretty well, but it's not doing that well with hotels. But I think you can see the point and also the status of the project. It's a really cool idea. It's just very early days. Now, there is one more way to interact with those agents that you might be interested in if you want to try this out for yourself. If I go into that samples.js directory and then I run this ADA CLI client and then I give it the URL of the agent that I want to talk to. In this case, we'll go with the hotel agent. We give it that hotel agent URL. This brings up an interactive terminal with that specific agent. Now, one interesting thing right away is we have this agent card that is returned to us and that gives all the specifics about that particular agent. There's more to it than what we're seeing here. I'll show you that in just a bit. But we can actually have a chat session with it right now. So, I can do / new to start a new task. That's a core feature of the ADA protocol. You have tasks with these agents. All right. I've used this ADA client CLI tool to make the request of tell me what hotels you have available to that hotels agent. and it's gotten back a reasonably decent response based on that demo data. Let's take a very quick look at one of these servers themselves to give you some background on that. I'm a JavaScript person, so that's what I used for this. We'll take a look at that hotel agent. As I mentioned before, this is just demo data that I created over in hotels.ts. But the really fun stuff is over in index.ts. So to start our server, we bring in the class for ADA server from the local server index.js file. That's actually a really important indicator about where this project is in terms of its deployment. There are no A to packages out in the node module space right now to support A to A. If you want to develop on this, you're literally going to be using the PC code for the protocol itself. Right down the bottom here, we instantiate that A to server with one of the most important things, which is our card. The card, and all AA servers have cards, is essentially the calling card of the agent. has the name, the description, the current URL, although I think you have gotten the URL, but whatever, it's okay. The version, the capabilities, whether it actually streams responses or not, and then an array of skills, and it's those skills that allow a multi- aent, that's an agent that talks to other agents to decide what agents to use and when. The rest of this sets up a handler, and that handler takes incoming requests and then passes them off to the AI of your choice. In this case, it uses Google's Genkit and the underlying model is Gemini 20 Flash. Of course, you get to process those messages however you want and send them to any model you want. If you want to send the O Lama, Open AI, hugging face, whatever you want to do, you can do that. All right. So, given what I know today, let's talk about what's good and bad with ADA. So, AAA uses JSON RPC as the schema to talk between the two ADA client and server instances. And I think that's a fantastic choice when it comes to overthe-wire communications. and it is what MCP uses. So, it's a fantastic analog to MCP. I love the idea that they have of an agent marketplace that there would be some registry where my multi- aent could go and say, \"Hey, give me an agent that does this. It would give me back a URL. It would get the card and start talking to the agent.\" I think that's a fantastic concept. I love this idea of this agent card that gives back to you either as a human or as another agent to see, is this going to actually do the kind of work that I want to do. Another great feature of ADA is it allows us to select the right model for the use case. Each one of these agents gets to decide which model it's going to use to service its request. In the original example with Claude and MCP, we're going to keep using that Claude sonet model for every one of our use cases and that might not be the right model. ADA allows us to say we want this model for this request and even have tuned models per agent. Authentication is built right into the protocol from the get-go. This is a huge advantage over what we got originally out of the box with MCP which didn't have any authentication but now does. I also like that they've learned other lessons from MCP. Like for example, ADA does not support the standard IO protocol. You can't just run one of these servers as a command. I think that's actually a really good way to go because if you had that then you wouldn't be able to do the registry or the agent marketplace concept that they were talking about. Also, I think the docs at this point in the evolution of the product are excellent. All right, so what's not so good? Well, the current code state, I would say the actual code docs need a lot of work. So, unless you're a developer that's familiar with both JavaScript and Python, it's going to be a while before you get this set up and running. And there's really not a lot of good feedback when it comes to what's wrong with your setup. So, you're going to need to do a lot of debugging and digging in, which for me is fun, but I know for a lot of folks is frustrating. I think reliability and testing is going to be a problem particularly as these agent networks grow. And then there's what I'm calling the multi- aent hairball. So what's that? Well, think about this multi- aent, the travel agent. The agent that's talking to other agents. Well, that agent, he's going to be really complex. As you saw when you're doing that interactive example in the browser, that agent is going to have to plan and connect with all these agents. it's going to be a lot of work to get that multi- aent right and there's going to be multiple different flavors of that multi- aent. So to me that's really a hairball and in particular where the agent is connecting with other agents and making requests of other agents your initial prompt to the agent is going to be from you but from following from then on that multi- aent is going to create its own prompts and be talking to agents on its own. So being able to trace those connections and see what prompts it's generated, we're going to need some serious work in there. And then finally, the idea of a centralized identity or billing system. If I have this registry and I'm going out and talking to all those agents, do I have to have unique contracts with each one of those? That's going to be something that's going to be interesting to see as this standard evolves. So will AA replace MCP? Well, I don't think so. MCP is a lower level standard. and it it presents tools to LLMs. We're always going to need those tools at that level of granularity. And ADA doesn't give us access to those tools at that level of granularity. And I know we've all had frustration with trying to get the prompts to do what we want them to do. So having MCP at that level of granularity, we can't replace that with A to A. So do we need A to A if we have MCP? Maybe, maybe not. Actually, I mean, if you've got a tool and that tool invokes an LLM to get stuff done and the new version of the MCB protocol can stream responses back, yeah, you could probably do something similar to what ADA is doing right now inside of MCP. But I do think that would be a misuse of MCP. And I think that having a specific protocol for agents to talk to other agents is valuable. But at the end of the day, I think as the docs say, ADA loves MCP. LLM plus tools are agents. MCP gives agents those tools. So that's why ADA and MCP play really nicely together. All right, if you have any questions or comments, be sure to put that in the comment section right down below. And in the meantime, if you like this video, hit that like button. If you really like the video, hit the subscribe button and click on that bell. You'll be notified the next time a new blue collar coder comes out.' metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "video_id = \"voaKr_JHvF4\"\n",
    "# ts = YouTubeTranscriptApi.list_transcripts(vid)\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "# ts = ytt_api.fetch(video_id, languages=['ko', 'en'])\n",
    "# print(list(ts))\n",
    "\n",
    "try:\n",
    "    transcript_list = ytt_api.list(video_id)\n",
    "    print(transcript_list)\n",
    "    try:\n",
    "        transcript = transcript_list.find_transcript(['ko']).fetch()\n",
    "    except:\n",
    "        transcript = transcript_list.find_transcript(['en']).fetch()\n",
    "except Exception as e:\n",
    "    print(\"자막을 가져올 수 없습니다:\", e)\n",
    "    transcript = []\n",
    "\n",
    "# print(transcript)\n",
    "\n",
    "# 2. transcript는 이런 리스트 형태\n",
    "# [{'text': '안녕하세요', 'start': 0.0, 'duration': 4.2}, ...]\n",
    "print(transcript[:2])\n",
    "\n",
    "# 3. 텍스트 병합 (원하는 방식으로)\n",
    "full_text = \" \".join([t.text for t in transcript])\n",
    "\n",
    "# 4. Document로 변환\n",
    "from langchain.docstore.document import Document\n",
    "doc = Document(\n",
    "    page_content=full_text,\n",
    "    metadata={\n",
    "        \"source\": video_url,\n",
    "        # \"language\": transcript.language if transcript else None,\n",
    "        # \"transcript\": transcript  # 필요하면 원본 JSON도 저장\n",
    "    }\n",
    ")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 영상 수: 5\n",
      "60분 이하 영상 수: 5\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1시간 이하인 영상만 고르기\n",
    "print('총 영상 수:', len(videos))\n",
    "# 영상 길이가 60분 이하인 영상만 남깁니다. \n",
    "videos = [v for v in videos if len(v['duration'].split(':')) < 3]\n",
    "print('60분 이하 영상 수:', len(videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Sl9EZpE61xA',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAI2p1dU9p9q97m8Tj2tyPewluofA',\n",
       "   'https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBpcJB5QG7qsRvGD7LAvzcWB_uUlg'],\n",
       "  'title': 'What is A2A (Agent to Agent Protocol)? | A2A Explained',\n",
       "  'long_desc': None,\n",
       "  'channel': 'codebasics',\n",
       "  'duration': '13:03',\n",
       "  'views': '조회수 33,009회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=Sl9EZpE61xA&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"Yesterday Google released agent to agent protocol and in this video I will provide a very intuitive explanation of what exactly it is. Obviously you need to know what are AI agents. If you don't I am linking couple of videos and here is a demo of Google agent space which uses A2A protocol. So let's say if you are an organization you are hiring a software engineer you can attach PDF file into this agent space and it will discover the agents which will help with the task. Okay so that is the first step in A2A protocol which is this particular client. Okay the the chatbot that you're seeing is able to discover other agents. So this chatbot itself is an agent but it is having this capability to discover other agents which can help it perform particular task. So it found this particular sourcing agent and then it is talking to that. Okay. So it is talking to that using a protocol. You see agent protocol. So agent protocol allows one agent to talk to other agent in a standardized way. So I will just show you the complete demo here. It uh talked to that other agent and it is asking you this clarifying question. Okay. Do you need a person in plus and minus 3 hours of US specific time? Uh so now you are getting into collaboration where clients and agent must support dynamic interaction where you can ask clarifying questions. Okay, what kind of requirements you have for this particular uh job requirement and and that is also part of that standard protocol. Okay. So you clarify uh your requirement here and then uh so this is that user experience negotiation and it will go to some platforms and it will find all these job profiles and you can just say please reach out to schedule interviews and maybe for interviews you can use multiple agents. So the idea here is as an end user you're talking to client which was that Google agent workspace and it has one base agent which is your remote agent and to accomplish a task it might be talking to couple of other agents and wherever you are seeing this green arrow that's where you are applying A2A protocol. It's very simple agent to agent protocol. Now you might wonder how is it different from model context protocol. And by the way if you don't know about MCP I have another video with a simple explanation. So let me give you an analogy to explain how this is different from MCP. Let's say you are a car repair guy. Uh in our LLM analogy this person is an LLM and to fix a tire or something you need all these tools right? See these are all the tools that you need to perform various tasks such as removing the screws, fixing the tire pressure and so on. And this entire system where you have LLM which is this car repair guy, he has all these tools and both of them are accomplishing certain task. So this entire system is called agent which can accomplish certain tasks such as fixing tire pressure or fixing whatever. Okay. And to communicate, LLM will use this MCP. So MCP is a protocol that helps LLM communicate with tools, resources, prompt, and so on. But in real life, agent will communicate with another agent. So if you're a car repair person, you might have to communicate with your customer or your part supplier and you will have often this backto back back and forth communication where you're saying okay customer is saying it shows a tire pressure warning. You will say okay I fixed it let me check again and you might find out that your pressure sensor is broken. then customer will say okay how much will it cost? So there is this back and forth. So part of the protocol is that you need to enable this back and forth communication in a standardized way. So to summarize A2A is a protocol which uh allows AI agents to communicate with each other in a standardized way. And I'm going to link a page from Google. Google came up with this particular protocol which is used by many companies by the way. Intuitit, lang chain, MongoDB, there are many companies which has started using this already. Now you need to know certain concept. The first one is agent card. So when this lady wants to repair her car, she has some way of discovering the right car repair person. And that discovery happens through agent card. Okay. So let's say she might has have different needs. For example, uh she need a person to repair her car and she might know a couple of car repair people. And then she will have information on a person who can help her sell the car. So car dealer. Okay. So she might have a directory where or a registry where she has all these agent cards and she knows the name of the person the description what kind of skills this person has. Entre mishra has skills of selling used cars, car financing whereas John Pandya has all these skills of oil change, tire repair and so on. So I'm showing uh the official documentation where they talk about discovering agent cards. So let's say if you are a client who is working on solving uh some task let's say finding software engineer you know that Google agents workspace demo that we saw now you need to have some way to discover what other agents you can work with just like that uh lady customer she has all that agent cards or agent tool registry agent registry rather where she knows that there is this person who is a car dealer, there is this person who is a repair person, there is this person who is the shop owner and so on. So similarly here there are multiple ways you can discover agents. The first way is you can use this particular agent.json file and clients will use DNS to resolve known or found domain. Okay. The other one is registry based where for an AI application here is the registry of trusted agents. Okay. And they're still working on this protocol. This is sort of like work in progress. Uh but there will be some kind of registry. There is a private discovery also. Okay. There are multiple ways where a client or the remote agent can discover other agents. Okay. Then comes the agent card. So this is once again a protocol. So you have a standardized way of showing your agent card and this is the typescript. This is the schema. So you should have agent card. Then you should have name, description, URL. So these these things are must. Okay, you understand what the protocol is. Protocol is basically defining that standard uh in which you are going to communicate. So if you are an agent uh and you want to send your agent card to other agent, it has to be in this format. Okay, it should have provider like which company it is. Okay, Google, whatever whatever that company is. U if you're writing an a a an agent which can uh help you buy groceries, then that company which is creating that agent that will be the organization name here. the capabilities and so on. Okay. Uh there is this task object also. So when you are giving a task to solve it has to be in this format. Okay. So there are different specification here. But let me just show you some sample. So if you scroll down this is an agent card for Google maps agent. So that is an agent which will help you with Google map related task. So this is the URL. This is the organization. These are the capabilities. See, it will have multiple skills. So, first skill is route planning. Second skill is custom map and so on. Then when you send a task, see, you will say tell me a joke. This is a different uh task by the way, not related to Google map. Uh but this is how you will specify your task and your response will come here and it is using JSON RPC as a communication protocol. Uh and as a data exchange format you are using JSON. So uh this is not like replacing HTTP or JSON RPC etc. The A2A protocol is built on top of HTTP, JSON RPC um you know some other protocols, bunch of other protocols which which I will talk about in this particular document. So you can uh read through all these uh sometimes you need you have streaming requirements. So you want to communicate through SSC. So A2A protocol also supports SSE and this is the example of that. Let's say you are writing a long paper. Okay, on some topic now you need to stream the response. So you can stream it like this. Okay, this is section one, section two and so on. And it supports multimodality as well. Uh it also supports error handling. And then here is a nice article on um A2A versus MCP and the garage example that I use. I I I got the inspiration from this. There are a bunch of diagrams that you can find it useful. Uh now let me show you the there is some code as well. So here there is a code code of an uh agent which is acting as a server. So it it will obviously uh display that uh agent card. So see agent card format is same as the schema that we saw. It has name, description, URL, the skills and skills is an array. So it can have multiple skills. So one of the skills is let's say this this is an example of a reimbursement tool. So your skill is you can process reimbursement tool. So if you are an organization which is building this agentic tool which can reimburse your expenses then you can use a A2A protocol and you can build one agent which is a reimbursement tool. Okay. So this is just the main server file. The actual implementation of reimbursement tool is here. So you will see a class which is your reimbursement agent. So this is an agent. An agent will have some capability. What is that capability? Well, it uses Gemini 2.0 flash model. It can do this thing. This is the prompt that it will use to pass the task over to an LLM. So, it is using some LLM. It is using tools. Of course, agents is nothing but LLM plus tools. Okay. So, this entire class is nothing but an agent. But this agent is communicating with other agents. Okay. It is providing all these capabilities through the agent card that I have these capabilities so that other agents can use this particular agent. All right. So I hope you understand uh now what exactly A2A is. I'm going to link all these documents for your detailed understanding. This was released just yesterday. So uh we'll have to see you know what kind of impact this has in the world of AI. It is still new lot of things are being evolved. So it it starts a new era for agent interoperability and it is going to be a very good development uh for companies and organizations which are building this agentic tools. All right. Thank you very much for watching. If you have any questions, post in the comment box below.\")]},\n",
       " {'id': 'WWHlehkRp3w',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDVSt8bALIgRKupS1h4d63Zr2dRxg',\n",
       "   'https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDkdltB-XhkieSR0Oq1XmLoCRqrLA'],\n",
       "  'title': \"Google's A2A Protocol in 100 Seconds (AI Agents)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Better Stack',\n",
       "  'duration': '2:26',\n",
       "  'views': '조회수 9,770회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=WWHlehkRp3w&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"AI agents are taking over, but they're not a hive mind yet. That's why Google and all these small companies have got together and worked on the A2A protocol, an open protocol built on top of standards like HTTP, SSE, and JSON RPC. If you've just caught up on MCP, don't worry. This isn't a competing protocol. It's a complimenting one. Agentic applications will need both. Here's a simplified use case. Imagine a company has hired someone new and HR has to tell the IT department to create an account, tell the finance department to set up payroll, and tell the facilities department to arrange a desk. Each department currently has their own AI agent to do this, but the agents don't talk to each other. So, HR wastes time sending emails back and forth. With A2A, all these agents speak the same language. HR just tells their main onboarding agent about the new hire, and this agent uses A2A to automatically tell the IT agent to create the accounts, the finance agent to set up pay, and the facilities agent to sort out the desk. Those individual agents might then use MCP servers to complete their tasks. Everything works smoothly with A2A. To be able to do this, the A2A protocol allows for a few key things. First, the client, like our onboarding agent, creates and communicates a task, and the remote agent completes it, like a manager delegating tasks to employees. Each agent has an agent card. This is just like a LinkedIn profile for an agent in JSON format that describes what each agent is good at. Its API endpoints and authentication needs so the client can pick the one that will do the task best. Side note, the protocol is secure by default supporting enterprisegrade authentication and authorization like OOTH or OIDC. Once it's picked the agent, A2A has built-in task management capabilities. A task has a life cycle. It can be immediate or longunning task. And each agent can sync on the completion status of the task like your daily standup meeting. The output of a task is known as an artifact with each message containing parts which are fully formed pieces of content like a generated image, iframes, video, web forms, and much much more. The agents will even be able to negotiate the user's UI capabilities to know if they can view this format. The final thing to know is the protocol is modality agnostic. It can do text but also streaming audio and video. Your AI meeting assistant could actually stream the meeting to the summarizer agent. So there you have it. A2A Google's bid to make the impending robot uprising a collaborative one. Now go and make your agents talk to each other. Comment down below what you think. And as always, see you in the next one.\")]},\n",
       " {'id': 'voaKr_JHvF4',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCcsxXPAhNn02WEnESMsRvhnXoglQ',\n",
       "   'https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCLHBbrCZWZhg1kQuDZV02U9lv2Ow'],\n",
       "  'title': \"What Makes Google's A2A Protocol REALLY POWERFUL (in 12 Minutes)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Jack Herrington',\n",
       "  'duration': '12:30',\n",
       "  'views': '조회수 55,512회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=voaKr_JHvF4&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content='The AI world is back with another three-letter acronym. Last time it was MCP or the model context protocol. This time it\\'s A2A from Google and it\\'s the agent to agent protocol. Let\\'s see what it is, how it works, and does it replace or complement MCP. Let\\'s get right into [Music] it. that understand this ADA protocol. It\\'s really good to understand MCP which is a related protocol. I\\'ve got a full overview video of that and there\\'s a link in the description right down below. Now before we begin, one little thing. This is a copyrighted video and if you\\'re not watching it on the blue color coder channel, you\\'re not in the right place and I do not consent to this being rebroadcast on any other platform. So let\\'s say that you want to book a flight. So you\\'ve got Claude desktop over here. Claude Desktop is an MCP client. It can connect to an MCP server from the airline and say, \"List tools. What do you have available for me to interact with your airline?\" The airline would send back, \"I\\'ve got tools to search flights and book flights.\" Your MCB client says, \"Hey, that\\'s great. I want you to search for a flight for Portland to Honolulu.\" It sends back some JSON data encoded as text. Claude then selects the flight that I want to go on to and then invokes the bookflight tool which returns, hey, I got a flight for you. What\\'s really going on here is that Claude is interacting with an LLM or a language learning model and invoking tools within that MCP server to get that done. So that\\'s the model contracts protocol and people nowadays are calling it the USBC for AIS. A toa is this agentto agent protocol. So let\\'s start talking about what an agent is. In the ADA protocol model, an agent is a combination of an LLM or large language model and tools. The agent takes an incoming task, uses the LLM to reason about that task, and then tools to execute on that task. And the ADA protocol allows either you or an agent to talk to another agent. So let\\'s take that example scenario of you booking a flight. Now, you would have access to some AA client. Now that\\'s actually not doable today. There are no kind of consumer A to A clients. All we have right now for A to A is this Google GitHub repo that has some sample code in it. It is not actually productized in any way. So this is what\\'s going to be in the future. So you\\'ve got your A to client. It has access to an LLM. It sends off a chat style message. Please book a flight. Here are the specifics of that flight. That agent that exists within that ATA server then uses its own LLM to parse that request and use the tools that we talked about before searching flights and booking flights internally to go and book that flight. And then it sends back a chat style response. Again, agents are LLMs plus tools. And this is agents talking with other agents across that A2A protocol. Now, if that wasn\\'t cool enough, the really interesting part about ADA is that I can connect with, for example, a travel agent agent. That travel agent would have its own LLM that it\\'s connected to, but it would also be connected to downstream agents like an airline agent, a hotel agent, and a car agent. So conceivably I could make a request like hey go and book an entire trip to Honolulu and it would figure out I need to go on this airline at this time go into this hotel rent this car and all that and the way that it\\'s going to do that is that that travel agent is going to use that same AA protocol to connect itself to the airline agent hotel agent and that car agent. So here\\'s the ADA site. It\\'s a really good read and you can also get some samples and in this case that is the Google ADA repo. Now that repo not only has example agents in both JavaScript and Python but it\\'s also got a demo application that you can connect to that acts as a multi- aent kind of like the travel agent in our example. It\\'s a single site that then connects to multiple agents simultaneously to try and get something done. So, I\\'ve actually built on top of that demo code and built a hotel agent and a flight agent that we can connect to on that demo. So, those are both running in my terminal. Here we have our hotel agent and here we have our flight agent and we\\'ve got the demo app running over here in our third terminal. So, let\\'s go into our browser, see how we go. This is really beta stuff, so this might work and might not. So, this is the demo UI and it\\'s actually currently hooked up to both those agents. So, let\\'s try it out. Let\\'s ask it to book a trip to Maui. All right, this is actually a pretty good response from what I\\'ve seen. So, it\\'s telling us it\\'s going to coordinate with both the flight agent and hotel agent. I apologize in advance for the CSS here. That\\'s was part of the demo. All right, I\\'ll give it a little more information and we\\'ll see what happens. All right, it looks like it\\'s interacting with the flight agent pretty well, but it\\'s not doing that well with hotels. But I think you can see the point and also the status of the project. It\\'s a really cool idea. It\\'s just very early days. Now, there is one more way to interact with those agents that you might be interested in if you want to try this out for yourself. If I go into that samples.js directory and then I run this ADA CLI client and then I give it the URL of the agent that I want to talk to. In this case, we\\'ll go with the hotel agent. We give it that hotel agent URL. This brings up an interactive terminal with that specific agent. Now, one interesting thing right away is we have this agent card that is returned to us and that gives all the specifics about that particular agent. There\\'s more to it than what we\\'re seeing here. I\\'ll show you that in just a bit. But we can actually have a chat session with it right now. So, I can do / new to start a new task. That\\'s a core feature of the ADA protocol. You have tasks with these agents. All right. I\\'ve used this ADA client CLI tool to make the request of tell me what hotels you have available to that hotels agent. and it\\'s gotten back a reasonably decent response based on that demo data. Let\\'s take a very quick look at one of these servers themselves to give you some background on that. I\\'m a JavaScript person, so that\\'s what I used for this. We\\'ll take a look at that hotel agent. As I mentioned before, this is just demo data that I created over in hotels.ts. But the really fun stuff is over in index.ts. So to start our server, we bring in the class for ADA server from the local server index.js file. That\\'s actually a really important indicator about where this project is in terms of its deployment. There are no A to packages out in the node module space right now to support A to A. If you want to develop on this, you\\'re literally going to be using the PC code for the protocol itself. Right down the bottom here, we instantiate that A to server with one of the most important things, which is our card. The card, and all AA servers have cards, is essentially the calling card of the agent. has the name, the description, the current URL, although I think you have gotten the URL, but whatever, it\\'s okay. The version, the capabilities, whether it actually streams responses or not, and then an array of skills, and it\\'s those skills that allow a multi- aent, that\\'s an agent that talks to other agents to decide what agents to use and when. The rest of this sets up a handler, and that handler takes incoming requests and then passes them off to the AI of your choice. In this case, it uses Google\\'s Genkit and the underlying model is Gemini 20 Flash. Of course, you get to process those messages however you want and send them to any model you want. If you want to send the O Lama, Open AI, hugging face, whatever you want to do, you can do that. All right. So, given what I know today, let\\'s talk about what\\'s good and bad with ADA. So, AAA uses JSON RPC as the schema to talk between the two ADA client and server instances. And I think that\\'s a fantastic choice when it comes to overthe-wire communications. and it is what MCP uses. So, it\\'s a fantastic analog to MCP. I love the idea that they have of an agent marketplace that there would be some registry where my multi- aent could go and say, \"Hey, give me an agent that does this. It would give me back a URL. It would get the card and start talking to the agent.\" I think that\\'s a fantastic concept. I love this idea of this agent card that gives back to you either as a human or as another agent to see, is this going to actually do the kind of work that I want to do. Another great feature of ADA is it allows us to select the right model for the use case. Each one of these agents gets to decide which model it\\'s going to use to service its request. In the original example with Claude and MCP, we\\'re going to keep using that Claude sonet model for every one of our use cases and that might not be the right model. ADA allows us to say we want this model for this request and even have tuned models per agent. Authentication is built right into the protocol from the get-go. This is a huge advantage over what we got originally out of the box with MCP which didn\\'t have any authentication but now does. I also like that they\\'ve learned other lessons from MCP. Like for example, ADA does not support the standard IO protocol. You can\\'t just run one of these servers as a command. I think that\\'s actually a really good way to go because if you had that then you wouldn\\'t be able to do the registry or the agent marketplace concept that they were talking about. Also, I think the docs at this point in the evolution of the product are excellent. All right, so what\\'s not so good? Well, the current code state, I would say the actual code docs need a lot of work. So, unless you\\'re a developer that\\'s familiar with both JavaScript and Python, it\\'s going to be a while before you get this set up and running. And there\\'s really not a lot of good feedback when it comes to what\\'s wrong with your setup. So, you\\'re going to need to do a lot of debugging and digging in, which for me is fun, but I know for a lot of folks is frustrating. I think reliability and testing is going to be a problem particularly as these agent networks grow. And then there\\'s what I\\'m calling the multi- aent hairball. So what\\'s that? Well, think about this multi- aent, the travel agent. The agent that\\'s talking to other agents. Well, that agent, he\\'s going to be really complex. As you saw when you\\'re doing that interactive example in the browser, that agent is going to have to plan and connect with all these agents. it\\'s going to be a lot of work to get that multi- aent right and there\\'s going to be multiple different flavors of that multi- aent. So to me that\\'s really a hairball and in particular where the agent is connecting with other agents and making requests of other agents your initial prompt to the agent is going to be from you but from following from then on that multi- aent is going to create its own prompts and be talking to agents on its own. So being able to trace those connections and see what prompts it\\'s generated, we\\'re going to need some serious work in there. And then finally, the idea of a centralized identity or billing system. If I have this registry and I\\'m going out and talking to all those agents, do I have to have unique contracts with each one of those? That\\'s going to be something that\\'s going to be interesting to see as this standard evolves. So will AA replace MCP? Well, I don\\'t think so. MCP is a lower level standard. and it it presents tools to LLMs. We\\'re always going to need those tools at that level of granularity. And ADA doesn\\'t give us access to those tools at that level of granularity. And I know we\\'ve all had frustration with trying to get the prompts to do what we want them to do. So having MCP at that level of granularity, we can\\'t replace that with A to A. So do we need A to A if we have MCP? Maybe, maybe not. Actually, I mean, if you\\'ve got a tool and that tool invokes an LLM to get stuff done and the new version of the MCB protocol can stream responses back, yeah, you could probably do something similar to what ADA is doing right now inside of MCP. But I do think that would be a misuse of MCP. And I think that having a specific protocol for agents to talk to other agents is valuable. But at the end of the day, I think as the docs say, ADA loves MCP. LLM plus tools are agents. MCP gives agents those tools. So that\\'s why ADA and MCP play really nicely together. All right, if you have any questions or comments, be sure to put that in the comment section right down below. And in the meantime, if you like this video, hit that like button. If you really like the video, hit the subscribe button and click on that bell. You\\'ll be notified the next time a new blue collar coder comes out.')]},\n",
       " {'id': 'GUozMSpnmcc',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAainynDDWkCS5D1lD9J_wiq90GfA',\n",
       "   'https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCL6gT6oBOergoFUfhxP4D1Rp7hKQ'],\n",
       "  'title': 'Google A2A Protocol Explained: Everything You Need to Know',\n",
       "  'long_desc': None,\n",
       "  'channel': 'AI LABS',\n",
       "  'duration': '8:20',\n",
       "  'views': '조회수 5,594회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"Google has introduced a new protocol called A2A which stands for agentto agent protocol. It enables communication between two agentic applications or between two agents. And the crazy part is that this protocol can connect AI agents from any framework whether it's Langchain, Crew AI, Google's ADK or even customuilt systems. A2A runs on a shared protocol built on HTTP. This protocol is honestly wild and the kind of applications and implementations we're going to see from it are going to be mind-blowing. Just like MCP, it's expected to gain a lot of traction. As more applications are developed with it, the momentum will only grow. The best part is that it doesn't replace MCP. Instead, it works alongside it and both can be used together to build powerful systems. Let's jump into the video and see how it all works. Right now, I'm going to show you an official demo from Google. This is exactly what they demonstrated. It starts with the end user which is you. The client in the demo is Google agent space but it could be any client. It begins with a single agent called the remote agent. Based on the task you give it, this agent looks for other agents to hand the task over to. This is where the A2A protocol steps in. It enables smooth communication between two AI agents. Now what exactly is an AI agent? They're just large language models with a set of tools. These tools define what the agent can do. In this new protocol, every agent has an agent card that describes its abilities. The remote agent reads the agent cards of other agents and picks the one best suited for the task. That agent can then pass the task on to another agent, creating a chain. This forms a multi- aent workflow. This is how A2A makes the process much easier. I'll show you in the GitHub repo how it works. You can connect agents from any framework and once this protocol is in place, those agents can talk to each other without friction. Now this is the demo that they have shared. The example they've provided shows the agentto agent protocol being used to hire a software engineer along with a job description. You can clearly observe how the A2A protocol functions. First the protocol is initiated which is visible in the thinking process. Then to discover different agents suitable for the task, it examines their agent cards. These are the main source for understanding the capabilities of each agent. There are several ways to explore agent cards. In this case, there's a tool registry where it finds the sourcing agent and initiates a call to it. You can see here that additional constraints have also been provided to the agent to find the best possible candidate. Once the sourcing agent completes its task, we can see it identifies five candidates for the job. If you continue watching the video, you'll notice that two weeks later, after the interviews are done, the agent is used again to gather updates and perform background checks. Based on the candidate data, the system is capable of running a background check on a single candidate. This entire process of hiring a software engineer based on a job description is automated using these agents. The most important thing is that this wasn't handled by a single agent. The A2A protocol allowed multiple agents to work together. all communicating through a single protocol. Let's clear up another concept about the agentto agent protocol and MCP. A2A is meant to work with MCP and even Google has confirmed this. To explain why, think of it like this. MCP is an LLM with tools or access to specific data. Picture it as a repair man. He has a screwdriver and the knowledge to fix cars. That's the MCP part. But this repair man also needs to talk to others. Maybe he needs to speak with clients or borrow parts from someone else. That's where the agentto aagent protocol comes in. It allows agents to communicate with each other. These agents could even be separate MCP servers acting as independent agents. They can share tools or request help when needed. The key to all of this is the agent card. It defines what each agent is capable of and helps them interact in a structured way. They've explained a clear connection between A2A and MCP. In their official documentation, it's outlined that future Agentic applications will need both of these protocols to become more capable. As an example, they use the same auto repair shop analogy. There are multiple handyman sub agents working in the shop. They need tools, but to use those tools properly, they also need extra context from customers. Those customers could be other agents. The interesting part is how MCP fits into this setup. We know that to identify an agent using the A2A protocol, it must have an agent card. These agent cards can be listed as resources. The MCP server can then provide access to them. The LLM fetches these agent cards and passes them to sub agents. Sub agents read the cards and based on the information, the LLM decides which external agent should be used. It's a clever integration and shows how both systems can work together in a flexible way. The agent card structure is clearly defined in their documentation. It includes key information about the AI agent such as the version, name, description, and its intended use. Then it lists the skills which are the core capabilities the agent can perform. It also shows the default content type the agent supports along with the parameters it accepts. Basically, it shows the kind of input the agent needs. For some agents, authentication might be required. So, specific document details would also be part of the agent card. When an LLM or another agent tries to access this agent, it first reads the agent card. Based on that, it decides whether to use the agent and how to interact with it. This makes the accuracy of the agent card critical to how the entire agentto agent system functions. Further in the documentation, they've provided sample agent cards and methods for sending tasks and receiving responses. One of the examples is a Google Maps agent. The card includes a clear description of the tasks it can perform along with the URL and provider details. It also specifies the type of authentication the agent needs. Below that, there's a format showing how a client can send a task to a remote agent to start a new job. In one simple example, the task is to tell a joke. The response comes back as a text output from a model which delivers the joke. This is one way to send a task and get a result. Other methods are also documented below. To get started, there's no need to memorize the syntax. You can feed the documentation into cursor with the at docs feature which will pick up the context and generate code accordingly. In the GitHub repo, they've included some sample agents that show how A2A agents can be implemented. One example is using Crew AI, a simple image generation agent that uses the Google Gemini API. You can install it easily. It's a basic agent that just runs on the A2A protocol. To get started, you need to clone the full GitHub repo because the commands depend on that structure. Once it's cloned, you can run the setup using a few simple commands. Just copy and paste them into your terminal. They're very straightforward. Once you run the command, it opens a command line interface for the A2A agent. The server passes it to the crew agent which uses the Gemini API to generate an image. That image is then returned to the server and finally back to the client. This is a simple implementation and it's not widely used just yet, but it will be. We saw the same thing happen with MCP. Adoption took some time, then it exploded. The same will happen here. Once people start building on top of it, AI agents will become incredibly powerful. They'll automate a huge amount of work and change how we use AI everyday. If you found this video helpful, subscribe to the channel. And if you'd like to support the content, check out the donation link in the description. Thanks for watching and I'll see you in the next video.\")]},\n",
       " {'id': 'IMcDEvXRBkY',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDQ5VbTviH_q8QXcX012o6KOSKl3w',\n",
       "   'https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCoS4vMaCArJgyD0UyAn-EEHhGJ4A'],\n",
       "  'title': 'MCP or A2A',\n",
       "  'long_desc': None,\n",
       "  'channel': \"John Savill's Technical Training\",\n",
       "  'duration': '19:59',\n",
       "  'views': '조회수 15,017회',\n",
       "  'publish_time': '1개월 전',\n",
       "  'url_suffix': '/watch?v=IMcDEvXRBkY&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content='Hey everyone. In this video, I want to dive into do I use MCP or A2A. There\\'s been a lot of conversation about what they do and when should I use which of them. So, I want to just quickly go over well what do they do? Then it should become apparent which do I use where and actually in a lot of scenarios you will use both of them. So when we think about our AI application, so I have my AI app and if we\\'re talking about generative AI, my AI application is going to be talking to a language model. This could be a large language model, a small language model, whatever that is. that hey this has been pre-trained on lots of information and based on those probability distributions it can generate token the next one the next one next one so our application gives it some prompt and then it will generate a response but we have to remember these language models are pre-trained on a certain body a corpus of knowledge and what it can respond to is really limited to what it was trained on which is finite in size might be vast but it\\'s still finite and it has a cut off time. It was only trained on things up to a certain date and so it can only generate a response based on its training and what you include in the prompt you send to it. And so to be really useful, we have to provide that large language model with additional knowledge with information about potential other actions that it could ask the app to perform on its behalf and then send the result. And so what we do to get that additional data is we use retrieve augmented generation. Maybe it\\'s graph rag where it focuses on the relationships between things. And so I may have this database. Maybe if we have a vector database in front of it to make it easy to find things based on what is the meaning of what we\\'re asking it, but it has this whole body of knowledge and then we can go and get additional information and we can then include it in the prompt. So now the model is not only data it was trained on but we\\'re giving it additional information that will help it in the goal we\\'re given. So hey, give me a summary of this research paper. We might include information about the research paper in it as well. And then we also give that knowledge and we may also have tools. So I might think well also I have some tool over here. And then once again what I would do as part of the prompt I would describe the tool. I would have to say hey the tool is capable of doing this and you can ask me to go and run things against that tool and then I\\'ll go and send you the response. So I would have to describe this tool as part of the prompt so it would know hey I can use this go and do this for me. The problem is twofold all these different resources and tools knowledge well they have their own way of communication. So my application would have to know well this protocol and this type of information I have to speak that would also have to go and be able to speak what the tool structure is and then I have to understand it enough to not only be able to communicate with them but then potentially describe them to the large language model so it knows when to potentially ask me to hey go and ask this knowledge for this piece of information or go and call this tool and send me the response. So it gets very challenging or potentially the providers have to go and write a whole bunch of code just for my particular app or platform that I\\'m running. So there\\'s there\\'s a lot of challenges with that. Hence the model context protocol or MCP. So this is a standard way that an AI application can go and interact with various types of knowledge, various types of tools. It can even get information about hey what are some good prompt structures that you can use to work with me. Now it uses a client server architecture. So the way MCP is going to work is there\\'s a standard client and it has a onetoone mapping. So I could think about okay my app is going to have an MCP client and this is the the same my app only ever talks hey to MCP to the client then there\\'ll be an MCP server now the MCP server speaks the MCP protocol between the client and the server. But this MCP server is specific to this particular resource. So it also speaks what\\'s required to go and speak to this knowledge store. So now me as the application I don\\'t need to worry about the specifics of this at all. I just speak MCP and there\\'s a standard library available. My MCP client talks to one of the number of MCP servers using the MCP protocol and then I can use this functionality. So it\\'s both the client piece. I\\'m going to write a server piece for my solution. Well, the service provider would write the MCP server and then I can talk it and it would work on this side as well. So again, there would be an MCP client that my app talks to. So there\\'s one client per MCP server I\\'m talking to. And then again, there\\'ll be an MCP server for this particular tool. And again, this MCP server speaks this tool format. So that tool provider writes an MCP server for their solution. this database or knowledge store writes an MCP server for their solution. And what\\'s great about this is it means well for the provider they only have to write a single MCP server. They don\\'t have to go and write a ton of different servers and plugins for all the different tools. They just write an MCP server. And so what\\'s great for now me as an app, I just have to speak MCP. I don\\'t care about the details of their particular protocols. is been abstracted for me. Now these MCP servers can actually live in different places because we do also have to think about security. So it\\'s possible the MCP server is provided by that provider but it actually runs locally on my box or it could be a remote MCP server. So there\\'s different ways that MCP works. If it\\'s local, they might give me a container format so I can just run it locally and then maybe I just use standard IO to talk to it. If it\\'s remote, then I\\'m going to use TLS encryption and server sent events to that communication. A lot of them do run locally today because initially in the specification, it was missing the bit about authentication which is pretty big for security, but that has been updated. So MCP servers now um act as an OOTH resource server and so I can use standard ways to do my authentication and token issuance. I can use a proper authorization service like entry ID or octo or any oorth compliant IDP. Whole point of all of this the MCP server is abstracting the specifics of how to talk and leverage any of these services. So now, hey, it\\'s really easy for my app to go and use all of these things. My app just has to talk MCP. Now, the other nice feature of MCP is reflection. My app can get the MCP client to ask, hey, what do you do? And it might say, well, hey, I provide resources. So, structured data and documents that can be sent to large language models. Think rag. So it could be, hey, I provide resources. It could be, hey, I provide tools. So they\\'re functions that I can call via the MCP to perform something. It can even give me a set of prompts that are predefined instructions or templates so it can do particular tasks. So what\\'s really nice about this is I have to describe it to the large language model. Hey, what are the things available? I don\\'t have to work that out with the reflection. They just tell my app, hey, here\\'s what I\\'m capable of. And then I just include that in my system prompt. So now large language model will know, oh, okay, I need you to call this or I need you to call this with this set of instructions. So it\\'s a really powerful capability. And the large language model doesn\\'t have to know anything about MCP. It just sees inputs. it returns outputs. It\\'s the app that\\'s actually just talking MCP and large language models already great at being told, hey, here\\'s some knowledge. Here are some actions. So, there\\'s really not a lot I would have to do. So, this simplifies the process of my app wanting to talk to other knowledge and tools and get help with what are the right prompt formats so that it can perform a better service and give the large language model more capability. So it\\'s providing a standard simple way for my AI app to integrate with other knowledge, other tools that can then be leveraged by the large language model. I don\\'t have to write my own interfaces. So that\\'s MCP. Hey, my app wants to talk to other stuff to help make my application more powerful, give the large language model more capabilities. And nearly every app you write is typically going to involve some additional knowledge and/or some additional tools. So nearly everything will probably want to take advantage of MCP. But often my AI application, well, it may actually be an agent. So maybe my app is actually an agent. And in the agent world, maybe it\\'s the human sort of prompted asking it to go something. Maybe it\\'s autonomous. And as part of an AI agent, we remember we like to keep them very focused on a particular task. We don\\'t like agents to have very broad sets of capabilities. It tends not to do well. It might start hallucinating. It may get confused about well, which knowledge should it use? So, an agent has a distinct task that it accomplishes using the knowledge and the tools that it has available to it. But if my agent has a very specific task, most likely then my agent is going to want to work with one or more other agents to complete my task. So if this is an agent, then very very likely, well, there\\'s an agent B, there\\'s an agent C, and they each do their own particular task. And again, remember all of these are probably using MCP. So maybe this has a particular set of tooling it uses and it\\'s using MCP to do that. Remember there\\'s it\\'s got his client, the server, etc., etc. Maybe there\\'s some set of knowledge. It\\'s talking MCP to do that. This agent, hey, maybe it\\'s just a set of knowledge. It\\'s going to use MCP anytime I want to be able to hook into other knowledge other tools. Hey, MCP is the standard simple way to do that. But these agents need to work together. Maybe it\\'s a task planner. My app is a task planner and the request is, hey, summarize a document and then schedule a meeting with the key points. So my task planner agent maybe processes the request but then it has to call this one to maybe summarize the document and then this gives it the key points when it has to send it to this one to go and book a meeting and populate the agenda. So now I\\'ve got requirements and it\\'s well how do I know the capabilities of these agents so I know which one to call and then how do I talk to it? So I have two challenges A to A as agent to agent solves this. So it provides the communication capability. So it provides the protocol. So when I think of A to A, what I\\'m doing here is A to A is the way that they can communicate. It is a JSON based protocol. I think it\\'s JSON RPC 2.0. It\\'s over HTTPS. Now, there are other options. I could use a message bus if I wanted to, but primarily it\\'s going to be that JSON based RP2 over HTTPS. With A to A, one is acting kind of like a client and one is acting like a server. One is initiating the request to another agent. as part of hey I need to you to perform some task. Now how do I know as this agent what this agent\\'s capabilities are and what this agent\\'s capabilities are? They have an agent card. Think of it like a human being when you meet someone. You used to give them a business card and it would say hey here\\'s who I am. Here\\'s how you can contact me. These are the methods of communication that I support. Telephone or fax, uh, email, whatever. It might have my job title on it. Maybe a description of the services I offer. And so it\\'s the same thing here. Hey, I initiate a communication. And as part of that, agent B is going to send me its cards. This will have its name. There might be a version. There\\'s going to be skills that it supports. There\\'s going to be capabilities it has like it supports streaming, it supports push notifications, etc. And so this is my agent card. And likewise, this is going to have one with its stuff on it. And I\\'m going to have one that I\\'ll share as well. So these can understand who I am. And they\\'re going to share their cards. So now by sharing the card, my agent understands its skills and it understands well what features it sports. Can I stream to it? Can I do a push notification to it. Now I know which agent to call to do what thing. It also includes the authentication mechanisms. Yes, authentication is actually a core part of the protocol and so it\\'s going to be a very simple way to start on boarding and utilizing this and that card is just a well-known URL. So on the server there\\'s like a well-known agent card URL I can use and we use the A2A protocol to communicate with them. So I have a way to communicate and I have a way to understand what their capabilities are. So my agent knows well which agent that are available to me should I be using. Now the interaction is going to be based around a certain task. So as I think about okay well I want to work and do something it\\'s based around a task that I need to complete and then the agents will exchange messages towards that. So they\\'re going to send messages potentially back and forth to say how they\\'re going. The messages could be text, images, audio, video, structured data like JSON or anything else. They may share context where it\\'s required. And these can actually be asynchronous interactions. So it could be a very longunning task. The agents don\\'t have to stay continuously connected. Again, it depends on their capabilities. It may be multi-turn as they collaborate on the task. There is a full task life cycle management available as part of the protocol. But then once the task is completed, they\\'re going to share artifacts. So the artifact is the end result of that. So by using A2A agent to agent, you\\'re removing the barriers between agents working together because otherwise I would have to work out, well, how can you talk to me? I would now have the same challenges as kind of over here, but how do I talk to this agent? How do I authenticate to this agent? How do I know what its capabilities are so I know when to do it? This standardizes all of that. It\\'s actually possible I might discover other agents because I\\'m being sent it by something else that says, \"Hey, there are these agents out there. You can go and leverage this.\" The key point, don\\'t think about it. Is it MCP or A2A? They\\'re doing very different things. Most likely you\\'re going to leverage both of them. So I use MCP when I as my app could be an agent wants to talk to other resources, other tools. So it lets me just understand MCP. I don\\'t have to worry about the specifics of protocol to go and actually then go and talk to a knowledge go and talk to a tool or anything else and they\\'re all likely to use MCP anytime the app wants to hook into something else. I use A2A when hey I\\'m an agent or maybe I\\'m an app and I want to call some agents and I want a standard way of doing that. I want a way that they understand that\\'s a standard protocol and it has the mechanism so I can understand well what does this agent actually do so I know when to talk to it and it\\'s not just when I know when to talk to it these agent cards I can then feed in as part of the prompts the last language model language model knows to tell my agent hey go and ask this agent to go and do this thing send me the result and then that\\'s how they can then go and work together. So hopefully that all makes sense. Don\\'t think of it as which do I use. It\\'s great other tools, other resources for my app MCP. I need to go and talk to other AI agents. Hey, I\\'ll use A to A and they all work very happily and live happily ever after. I hope that helped clear it up and until next video, take care.')]}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi()\n",
    "for v in videos:\n",
    "    video_id = v['id']\n",
    "    try:\n",
    "        transcript_list = ytt_api.list(video_id)\n",
    "        try:\n",
    "            transcript = transcript_list.find_transcript(['ko']).fetch()\n",
    "        except:\n",
    "            transcript = transcript_list.find_transcript(['en']).fetch()\n",
    "    except Exception as e:\n",
    "        print(\"자막을 가져올 수 없습니다:\", e)\n",
    "        transcript = []\n",
    "\n",
    "    # 3. 텍스트 병합 (원하는 방식으로)\n",
    "    full_text = \" \".join([t.text for t in transcript])\n",
    "\n",
    "    # 4. Document로 변환\n",
    "    from langchain.docstore.document import Document\n",
    "    doc = Document(\n",
    "        page_content=full_text,\n",
    "        metadata={\n",
    "            \"source\": video_url,\n",
    "            # \"language\": transcript.language if transcript else None,\n",
    "            # \"transcript\": transcript  # 필요하면 원본 JSON도 저장\n",
    "        }\n",
    "    )\n",
    "\n",
    "    v['content'] = [doc]\n",
    "\n",
    "videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자막 내용 요약하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 10, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C4qcMj2eJSxeWWaH6nyNssZoAdLFP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fdaf9ddf-31c7-490d-9c18-cea2cdfb294c-0', usage_metadata={'input_tokens': 10, 'output_tokens': 10, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 언어 모델 설정\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model.invoke(\"안녕?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"Yesterday Google released agent to agent protocol and in this video I will provide a very intuitive explanation of what exactly it is. Obviously you need to know what are AI agents. If you don't I am linking couple of videos and here is a demo of Google agent space which uses A2A protocol. So let's say if you are an organization you are hiring a software engineer you can attach PDF file into this agent space and it will discover the agents which will help with the task. Okay so that is the first step in A2A protocol which is this particular client. Okay the the chatbot that you're seeing is able to discover other agents. So this chatbot itself is an agent but it is having this capability to discover other agents which can help it perform particular task. So it found this particular sourcing agent and then it is talking to that. Okay. So it is talking to that using a protocol. You see agent protocol. So agent protocol allows one agent to talk to other agent in a standardized way. So I will just show you the complete demo here. It uh talked to that other agent and it is asking you this clarifying question. Okay. Do you need a person in plus and minus 3 hours of US specific time? Uh so now you are getting into collaboration where clients and agent must support dynamic interaction where you can ask clarifying questions. Okay, what kind of requirements you have for this particular uh job requirement and and that is also part of that standard protocol. Okay. So you clarify uh your requirement here and then uh so this is that user experience negotiation and it will go to some platforms and it will find all these job profiles and you can just say please reach out to schedule interviews and maybe for interviews you can use multiple agents. So the idea here is as an end user you're talking to client which was that Google agent workspace and it has one base agent which is your remote agent and to accomplish a task it might be talking to couple of other agents and wherever you are seeing this green arrow that's where you are applying A2A protocol. It's very simple agent to agent protocol. Now you might wonder how is it different from model context protocol. And by the way if you don't know about MCP I have another video with a simple explanation. So let me give you an analogy to explain how this is different from MCP. Let's say you are a car repair guy. Uh in our LLM analogy this person is an LLM and to fix a tire or something you need all these tools right? See these are all the tools that you need to perform various tasks such as removing the screws, fixing the tire pressure and so on. And this entire system where you have LLM which is this car repair guy, he has all these tools and both of them are accomplishing certain task. So this entire system is called agent which can accomplish certain tasks such as fixing tire pressure or fixing whatever. Okay. And to communicate, LLM will use this MCP. So MCP is a protocol that helps LLM communicate with tools, resources, prompt, and so on. But in real life, agent will communicate with another agent. So if you're a car repair person, you might have to communicate with your customer or your part supplier and you will have often this backto back back and forth communication where you're saying okay customer is saying it shows a tire pressure warning. You will say okay I fixed it let me check again and you might find out that your pressure sensor is broken. then customer will say okay how much will it cost? So there is this back and forth. So part of the protocol is that you need to enable this back and forth communication in a standardized way. So to summarize A2A is a protocol which uh allows AI agents to communicate with each other in a standardized way. And I'm going to link a page from Google. Google came up with this particular protocol which is used by many companies by the way. Intuitit, lang chain, MongoDB, there are many companies which has started using this already. Now you need to know certain concept. The first one is agent card. So when this lady wants to repair her car, she has some way of discovering the right car repair person. And that discovery happens through agent card. Okay. So let's say she might has have different needs. For example, uh she need a person to repair her car and she might know a couple of car repair people. And then she will have information on a person who can help her sell the car. So car dealer. Okay. So she might have a directory where or a registry where she has all these agent cards and she knows the name of the person the description what kind of skills this person has. Entre mishra has skills of selling used cars, car financing whereas John Pandya has all these skills of oil change, tire repair and so on. So I'm showing uh the official documentation where they talk about discovering agent cards. So let's say if you are a client who is working on solving uh some task let's say finding software engineer you know that Google agents workspace demo that we saw now you need to have some way to discover what other agents you can work with just like that uh lady customer she has all that agent cards or agent tool registry agent registry rather where she knows that there is this person who is a car dealer, there is this person who is a repair person, there is this person who is the shop owner and so on. So similarly here there are multiple ways you can discover agents. The first way is you can use this particular agent.json file and clients will use DNS to resolve known or found domain. Okay. The other one is registry based where for an AI application here is the registry of trusted agents. Okay. And they're still working on this protocol. This is sort of like work in progress. Uh but there will be some kind of registry. There is a private discovery also. Okay. There are multiple ways where a client or the remote agent can discover other agents. Okay. Then comes the agent card. So this is once again a protocol. So you have a standardized way of showing your agent card and this is the typescript. This is the schema. So you should have agent card. Then you should have name, description, URL. So these these things are must. Okay, you understand what the protocol is. Protocol is basically defining that standard uh in which you are going to communicate. So if you are an agent uh and you want to send your agent card to other agent, it has to be in this format. Okay, it should have provider like which company it is. Okay, Google, whatever whatever that company is. U if you're writing an a a an agent which can uh help you buy groceries, then that company which is creating that agent that will be the organization name here. the capabilities and so on. Okay. Uh there is this task object also. So when you are giving a task to solve it has to be in this format. Okay. So there are different specification here. But let me just show you some sample. So if you scroll down this is an agent card for Google maps agent. So that is an agent which will help you with Google map related task. So this is the URL. This is the organization. These are the capabilities. See, it will have multiple skills. So, first skill is route planning. Second skill is custom map and so on. Then when you send a task, see, you will say tell me a joke. This is a different uh task by the way, not related to Google map. Uh but this is how you will specify your task and your response will come here and it is using JSON RPC as a communication protocol. Uh and as a data exchange format you are using JSON. So uh this is not like replacing HTTP or JSON RPC etc. The A2A protocol is built on top of HTTP, JSON RPC um you know some other protocols, bunch of other protocols which which I will talk about in this particular document. So you can uh read through all these uh sometimes you need you have streaming requirements. So you want to communicate through SSC. So A2A protocol also supports SSE and this is the example of that. Let's say you are writing a long paper. Okay, on some topic now you need to stream the response. So you can stream it like this. Okay, this is section one, section two and so on. And it supports multimodality as well. Uh it also supports error handling. And then here is a nice article on um A2A versus MCP and the garage example that I use. I I I got the inspiration from this. There are a bunch of diagrams that you can find it useful. Uh now let me show you the there is some code as well. So here there is a code code of an uh agent which is acting as a server. So it it will obviously uh display that uh agent card. So see agent card format is same as the schema that we saw. It has name, description, URL, the skills and skills is an array. So it can have multiple skills. So one of the skills is let's say this this is an example of a reimbursement tool. So your skill is you can process reimbursement tool. So if you are an organization which is building this agentic tool which can reimburse your expenses then you can use a A2A protocol and you can build one agent which is a reimbursement tool. Okay. So this is just the main server file. The actual implementation of reimbursement tool is here. So you will see a class which is your reimbursement agent. So this is an agent. An agent will have some capability. What is that capability? Well, it uses Gemini 2.0 flash model. It can do this thing. This is the prompt that it will use to pass the task over to an LLM. So, it is using some LLM. It is using tools. Of course, agents is nothing but LLM plus tools. Okay. So, this entire class is nothing but an agent. But this agent is communicating with other agents. Okay. It is providing all these capabilities through the agent card that I have these capabilities so that other agents can use this particular agent. All right. So I hope you understand uh now what exactly A2A is. I'm going to link all these documents for your detailed understanding. This was released just yesterday. So uh we'll have to see you know what kind of impact this has in the world of AI. It is still new lot of things are being evolved. So it it starts a new era for agent interoperability and it is going to be a very good development uh for companies and organizations which are building this agentic tools. All right. Thank you very much for watching. If you have any questions, post in the comment box below.\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어제 Google이 에이전트 간 통신 프로토콜(A2A)을 출시했습니다. 이 비디오에서는 해당 프로토콜이 무엇인지에 대한 직관적인 설명을 제공합니다. AI 에이전트를 이해해야 하며, 이에 대한 소개 영상도 링크되어 있습니다.\\n\\nA2A 프로토콜은 한 에이전트가 다른 에이전트와 표준화된 방식으로 소통할 수 있게 해줍니다. 예를 들어, 조직에서 소프트웨어 엔지니어를 채용하는 경우, PDF 파일을 에이전트 공간에 첨부하면, 이 에이전트가 필요한 작업을 도와줄 다른 에이전트를 찾아냅니다. 이 챗봇은 다른 에이전트를 발견하고, 필요한 경우 명확한 질문을 하며 협력 과정을 거칩니다. \\n\\n이 프로토콜은 고객과 에이전트 간의 동적 상호작용을 지원하며, 여러 에이전트가 협력할 수 있도록 합니다. A2A 프로토콜은 AI 에이전트가 서로 통신하기 위한 표준 프로토콜로 정의되어 있습니다.\\n\\n이 비디오에서는 A2A와 모델 컨텍스트 프로토콜(MCP)의 차이를 설명하기 위해 자동차 수리사례를 예로 들고 있습니다. MCP는 LLM이 도구 및 자원과 통신하는 데 도움을 주는 프로토콜인 반면, A2A는 에이전트 간의 통신을 다룹니다.\\n\\nA2A 프로토콜의 필수 개념 중 하나는 에이전트 카드입니다. 고객이 적합한 수리사를 찾기 위해 에이전트 카드를 통해 필요한 정보를 검색하는 방식과 유사합니다. 다양한 에이전트를 발견하는 방법도 여러 가지가 있으며, 각각의 에이전트는 특정 형식의 에이전트 카드를 가지고 있습니다.\\n\\n이 프로토콜은 Google 및 여러 기업들에 의해 사용되고 있으며, 에이전트 간 상호 운용성을 보장하기 위해 표준화된 방식으로 에이전트를 소통하게 해줍니다. A2A 프로토콜은 HTTP 및 JSON RPC 등의 기존 프로토콜 위에 구축되어 있으며, 스트리밍 요구 사항 및 오류 처리도 지원합니다.\\n\\n영상은 A2A 프로토콜의 작동 방식을 설명하고, 코드 예제와 함께 에이전트의 기능을 보여줍니다. A2A의 출시는 인공지능 세계에 새로운 시대를 여는 중요한 발전으로 평가받고 있습니다.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영상 하나만 요약하기\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# 동영상 요약 프롬프트 작성\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"다음 영상에 대한 요약을 한국어로 만들어줘 :\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "result = chain.invoke({\"context\": videos[0]['content']})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:49<00:00,  9.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'Sl9EZpE61xA',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAI2p1dU9p9q97m8Tj2tyPewluofA',\n",
       "   'https://i.ytimg.com/vi/Sl9EZpE61xA/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBpcJB5QG7qsRvGD7LAvzcWB_uUlg'],\n",
       "  'title': 'What is A2A (Agent to Agent Protocol)? | A2A Explained',\n",
       "  'long_desc': None,\n",
       "  'channel': 'codebasics',\n",
       "  'duration': '13:03',\n",
       "  'views': '조회수 33,009회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=Sl9EZpE61xA&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"Yesterday Google released agent to agent protocol and in this video I will provide a very intuitive explanation of what exactly it is. Obviously you need to know what are AI agents. If you don't I am linking couple of videos and here is a demo of Google agent space which uses A2A protocol. So let's say if you are an organization you are hiring a software engineer you can attach PDF file into this agent space and it will discover the agents which will help with the task. Okay so that is the first step in A2A protocol which is this particular client. Okay the the chatbot that you're seeing is able to discover other agents. So this chatbot itself is an agent but it is having this capability to discover other agents which can help it perform particular task. So it found this particular sourcing agent and then it is talking to that. Okay. So it is talking to that using a protocol. You see agent protocol. So agent protocol allows one agent to talk to other agent in a standardized way. So I will just show you the complete demo here. It uh talked to that other agent and it is asking you this clarifying question. Okay. Do you need a person in plus and minus 3 hours of US specific time? Uh so now you are getting into collaboration where clients and agent must support dynamic interaction where you can ask clarifying questions. Okay, what kind of requirements you have for this particular uh job requirement and and that is also part of that standard protocol. Okay. So you clarify uh your requirement here and then uh so this is that user experience negotiation and it will go to some platforms and it will find all these job profiles and you can just say please reach out to schedule interviews and maybe for interviews you can use multiple agents. So the idea here is as an end user you're talking to client which was that Google agent workspace and it has one base agent which is your remote agent and to accomplish a task it might be talking to couple of other agents and wherever you are seeing this green arrow that's where you are applying A2A protocol. It's very simple agent to agent protocol. Now you might wonder how is it different from model context protocol. And by the way if you don't know about MCP I have another video with a simple explanation. So let me give you an analogy to explain how this is different from MCP. Let's say you are a car repair guy. Uh in our LLM analogy this person is an LLM and to fix a tire or something you need all these tools right? See these are all the tools that you need to perform various tasks such as removing the screws, fixing the tire pressure and so on. And this entire system where you have LLM which is this car repair guy, he has all these tools and both of them are accomplishing certain task. So this entire system is called agent which can accomplish certain tasks such as fixing tire pressure or fixing whatever. Okay. And to communicate, LLM will use this MCP. So MCP is a protocol that helps LLM communicate with tools, resources, prompt, and so on. But in real life, agent will communicate with another agent. So if you're a car repair person, you might have to communicate with your customer or your part supplier and you will have often this backto back back and forth communication where you're saying okay customer is saying it shows a tire pressure warning. You will say okay I fixed it let me check again and you might find out that your pressure sensor is broken. then customer will say okay how much will it cost? So there is this back and forth. So part of the protocol is that you need to enable this back and forth communication in a standardized way. So to summarize A2A is a protocol which uh allows AI agents to communicate with each other in a standardized way. And I'm going to link a page from Google. Google came up with this particular protocol which is used by many companies by the way. Intuitit, lang chain, MongoDB, there are many companies which has started using this already. Now you need to know certain concept. The first one is agent card. So when this lady wants to repair her car, she has some way of discovering the right car repair person. And that discovery happens through agent card. Okay. So let's say she might has have different needs. For example, uh she need a person to repair her car and she might know a couple of car repair people. And then she will have information on a person who can help her sell the car. So car dealer. Okay. So she might have a directory where or a registry where she has all these agent cards and she knows the name of the person the description what kind of skills this person has. Entre mishra has skills of selling used cars, car financing whereas John Pandya has all these skills of oil change, tire repair and so on. So I'm showing uh the official documentation where they talk about discovering agent cards. So let's say if you are a client who is working on solving uh some task let's say finding software engineer you know that Google agents workspace demo that we saw now you need to have some way to discover what other agents you can work with just like that uh lady customer she has all that agent cards or agent tool registry agent registry rather where she knows that there is this person who is a car dealer, there is this person who is a repair person, there is this person who is the shop owner and so on. So similarly here there are multiple ways you can discover agents. The first way is you can use this particular agent.json file and clients will use DNS to resolve known or found domain. Okay. The other one is registry based where for an AI application here is the registry of trusted agents. Okay. And they're still working on this protocol. This is sort of like work in progress. Uh but there will be some kind of registry. There is a private discovery also. Okay. There are multiple ways where a client or the remote agent can discover other agents. Okay. Then comes the agent card. So this is once again a protocol. So you have a standardized way of showing your agent card and this is the typescript. This is the schema. So you should have agent card. Then you should have name, description, URL. So these these things are must. Okay, you understand what the protocol is. Protocol is basically defining that standard uh in which you are going to communicate. So if you are an agent uh and you want to send your agent card to other agent, it has to be in this format. Okay, it should have provider like which company it is. Okay, Google, whatever whatever that company is. U if you're writing an a a an agent which can uh help you buy groceries, then that company which is creating that agent that will be the organization name here. the capabilities and so on. Okay. Uh there is this task object also. So when you are giving a task to solve it has to be in this format. Okay. So there are different specification here. But let me just show you some sample. So if you scroll down this is an agent card for Google maps agent. So that is an agent which will help you with Google map related task. So this is the URL. This is the organization. These are the capabilities. See, it will have multiple skills. So, first skill is route planning. Second skill is custom map and so on. Then when you send a task, see, you will say tell me a joke. This is a different uh task by the way, not related to Google map. Uh but this is how you will specify your task and your response will come here and it is using JSON RPC as a communication protocol. Uh and as a data exchange format you are using JSON. So uh this is not like replacing HTTP or JSON RPC etc. The A2A protocol is built on top of HTTP, JSON RPC um you know some other protocols, bunch of other protocols which which I will talk about in this particular document. So you can uh read through all these uh sometimes you need you have streaming requirements. So you want to communicate through SSC. So A2A protocol also supports SSE and this is the example of that. Let's say you are writing a long paper. Okay, on some topic now you need to stream the response. So you can stream it like this. Okay, this is section one, section two and so on. And it supports multimodality as well. Uh it also supports error handling. And then here is a nice article on um A2A versus MCP and the garage example that I use. I I I got the inspiration from this. There are a bunch of diagrams that you can find it useful. Uh now let me show you the there is some code as well. So here there is a code code of an uh agent which is acting as a server. So it it will obviously uh display that uh agent card. So see agent card format is same as the schema that we saw. It has name, description, URL, the skills and skills is an array. So it can have multiple skills. So one of the skills is let's say this this is an example of a reimbursement tool. So your skill is you can process reimbursement tool. So if you are an organization which is building this agentic tool which can reimburse your expenses then you can use a A2A protocol and you can build one agent which is a reimbursement tool. Okay. So this is just the main server file. The actual implementation of reimbursement tool is here. So you will see a class which is your reimbursement agent. So this is an agent. An agent will have some capability. What is that capability? Well, it uses Gemini 2.0 flash model. It can do this thing. This is the prompt that it will use to pass the task over to an LLM. So, it is using some LLM. It is using tools. Of course, agents is nothing but LLM plus tools. Okay. So, this entire class is nothing but an agent. But this agent is communicating with other agents. Okay. It is providing all these capabilities through the agent card that I have these capabilities so that other agents can use this particular agent. All right. So I hope you understand uh now what exactly A2A is. I'm going to link all these documents for your detailed understanding. This was released just yesterday. So uh we'll have to see you know what kind of impact this has in the world of AI. It is still new lot of things are being evolved. So it it starts a new era for agent interoperability and it is going to be a very good development uh for companies and organizations which are building this agentic tools. All right. Thank you very much for watching. If you have any questions, post in the comment box below.\")],\n",
       "  'summary': '어제 구글이 에이전트 간 프로토콜(A2A 프로토콜)을 발표했습니다. 이 영상에서는 A2A 프로토콜이 무엇인지 직관적으로 설명하고 있습니다. 먼저, AI 에이전트가 무엇인지 이해하는 것이 중요합니다. 영상에서는 구글의 에이전트 스페이스 데모를 통해 A2A 프로토콜이 어떻게 작동하는지 보여줍니다. 예를 들어, 조직에서 소프트웨어 엔지니어를 고용하려고 할 때, 해당 에이전트 스페이스에 PDF 파일을 첨부하면 에이전트들이 해당 작업을 도와줄 수 있습니다.\\n\\nA2A 프로토콜은 하나의 에이전트가 다른 에이전트와 표준화된 방식으로 소통할 수 있게 해줍니다. 이 과정에서 에이전트는 필요한 정보를 확인하기 위해 서로 질문하고 응답하는 협업을 진행하는데, 이것이 동적 상호작용을 지원합니다. 영상에서는 A2A 프로토콜과 모델 컨텍스트 프로토콜(MCP)의 차이점을 설명하기 위해 자동차 수리 기술자를 비유로 듭니다. MCP는 LLM(대규모 언어 모델)과 도구 간의 커뮤니케이션을 돕는 프로토콜인 반면, A2A는 에이전트 간의 커뮤니케이션을 다룹니다.\\n\\n또한, 에이전트 카드를 통해 에이전트 발견이 이루어지며, 에이전트 카드의 표준 형식이 중요하다고 강조합니다. 예를 들어, 에이전트 카드는 이름, 설명, URL 등의 정보를 포함해야 합니다. A2A 프로토콜은 여러 기업에서 사용되고 있으며, 이들의 문서와 샘플 코드도 제공됩니다. 결국, A2A 프로토콜은 AI 에이전트 간의 상호 운용성을 높여줄 것이며, 에이전트 도구를 개발하는 기업들에게 긍정적인 영향을 미칠 것으로 기대됩니다. \\n\\n영상 시청 후 궁금한 사항은 댓글로 남길 수 있습니다.'},\n",
       " {'id': 'WWHlehkRp3w',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDVSt8bALIgRKupS1h4d63Zr2dRxg',\n",
       "   'https://i.ytimg.com/vi/WWHlehkRp3w/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDkdltB-XhkieSR0Oq1XmLoCRqrLA'],\n",
       "  'title': \"Google's A2A Protocol in 100 Seconds (AI Agents)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Better Stack',\n",
       "  'duration': '2:26',\n",
       "  'views': '조회수 9,770회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=WWHlehkRp3w&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"AI agents are taking over, but they're not a hive mind yet. That's why Google and all these small companies have got together and worked on the A2A protocol, an open protocol built on top of standards like HTTP, SSE, and JSON RPC. If you've just caught up on MCP, don't worry. This isn't a competing protocol. It's a complimenting one. Agentic applications will need both. Here's a simplified use case. Imagine a company has hired someone new and HR has to tell the IT department to create an account, tell the finance department to set up payroll, and tell the facilities department to arrange a desk. Each department currently has their own AI agent to do this, but the agents don't talk to each other. So, HR wastes time sending emails back and forth. With A2A, all these agents speak the same language. HR just tells their main onboarding agent about the new hire, and this agent uses A2A to automatically tell the IT agent to create the accounts, the finance agent to set up pay, and the facilities agent to sort out the desk. Those individual agents might then use MCP servers to complete their tasks. Everything works smoothly with A2A. To be able to do this, the A2A protocol allows for a few key things. First, the client, like our onboarding agent, creates and communicates a task, and the remote agent completes it, like a manager delegating tasks to employees. Each agent has an agent card. This is just like a LinkedIn profile for an agent in JSON format that describes what each agent is good at. Its API endpoints and authentication needs so the client can pick the one that will do the task best. Side note, the protocol is secure by default supporting enterprisegrade authentication and authorization like OOTH or OIDC. Once it's picked the agent, A2A has built-in task management capabilities. A task has a life cycle. It can be immediate or longunning task. And each agent can sync on the completion status of the task like your daily standup meeting. The output of a task is known as an artifact with each message containing parts which are fully formed pieces of content like a generated image, iframes, video, web forms, and much much more. The agents will even be able to negotiate the user's UI capabilities to know if they can view this format. The final thing to know is the protocol is modality agnostic. It can do text but also streaming audio and video. Your AI meeting assistant could actually stream the meeting to the summarizer agent. So there you have it. A2A Google's bid to make the impending robot uprising a collaborative one. Now go and make your agents talk to each other. Comment down below what you think. And as always, see you in the next one.\")],\n",
       "  'summary': 'AI 에이전트들이 점점 더 많이 사용되고 있지만, 아직까지는 hive mind(벌집 지성)이 되지 않았습니다. 그래서 구글과 여러 소규모 회사들이 A2A 프로토콜을 함께 개발했습니다. 이 프로토콜은 HTTP, SSE, JSON RPC와 같은 표준 위에 구축된 오픈 프로토콜입니다. MCP(Multi-Chain Protocol)에 대해 알게 된 적이 있다면 걱정하지 마세요. 이 프로토콜은 경쟁하는 것이 아니라 보완하는 역할을 합니다. 에이전트 애플리케이션은 둘 다 필요합니다.\\n\\n예를 들어, 한 회사가 신입 사원을 고용했다고 가정해봅시다. 인사부는 IT 부서에 계정을 만들도록, 재무부에는 급여 설정을, 시설부에는 책상을 마련해 달라고 각각 알려야 합니다. 현재 각 부서에는 각각의 AI 에이전트가 존재하지만, 이 에이전트들은 서로 소통하지 않습니다. 결과적으로 인사부는 이메일을 주고받느라 시간을 낭비합니다. A2A 프로토콜이 있으면 모든 에이전트가 동일한 언어로 소통할 수 있습니다. 인사부는 주요 온보딩 에이전트에게 신입에 대한 정보를 제공하면, 이 에이전트는 A2A를 사용하여 IT 에이전트에게 계정 생성을, 재무 에이전트에게 급여 설정을, 시설 부서 에이전트에게 책상 정리를 자동으로 알립니다. 각 에이전트는 자신의 작업을 완료하기 위해 MCP 서버를 사용할 수 있습니다. A2A 덕분에 모든 작업이 원활하게 진행됩니다.\\n\\nA2A 프로토콜은 몇 가지 핵심 기능을 지원합니다. 첫 번째로, 클라이언트인 온보딩 에이전트가 작업을 생성하고 이를 원격 에이전트가 완료하는 방식으로, 마치 관리자들이 직원에게 작업을 위임하는 것과 같습니다. 각 에이전트는 자신의 강점을 설명하는 JSON 형식의 에이전트 카드가 있어, 클라이언트가 어떤 에이전트가 작업을 가장 잘 수행할지를 선택할 수 있습니다. 이 프로토콜은 기본적으로 보안이 강화되어 있으며, 기업급 인증과 권한 부여를 지원합니다.\\n\\n작업이 완료되면 A2A는 내장된 작업 관리 기능을 통해 작업의 생애 주기를 관리할 수 있습니다. 각 작업은 즉시 수행되거나 장기적인 작업일 수 있으며, 각 에이전트는 작업 완료 상태를 동기화할 수 있습니다. 작업의 출력 결과는 아티팩트로 알려져 있으며, 각 메시지에는 생성된 이미지, iframe, 비디오, 웹 양식 등 완전한 형식의 콘텐츠 조각들이 포함됩니다. 에이전트들은 또한 사용자의 UI 기능을 협상할 수 있습니다.\\n\\n마지막으로, 이 프로토콜은 모달리티에 구애받지 않으며, 텍스트뿐 아니라 오디오와 비디오 스트리밍도 지원합니다. AI 미팅 어시스턴트가 회의를 요약 에이전트에게 스트리밍할 수 있습니다. A2A는 구글이 로봇의 반란이 다함께 협력하는 방식이 되도록 하려는 시도입니다. 이제 여러분의 에이전트들이 서로 소통하도록 만들어보세요. 여러분의 생각을 댓글로 남겨주세요. 다음 영상에서 만나요!'},\n",
       " {'id': 'voaKr_JHvF4',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLCcsxXPAhNn02WEnESMsRvhnXoglQ',\n",
       "   'https://i.ytimg.com/vi/voaKr_JHvF4/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCLHBbrCZWZhg1kQuDZV02U9lv2Ow'],\n",
       "  'title': \"What Makes Google's A2A Protocol REALLY POWERFUL (in 12 Minutes)\",\n",
       "  'long_desc': None,\n",
       "  'channel': 'Jack Herrington',\n",
       "  'duration': '12:30',\n",
       "  'views': '조회수 55,512회',\n",
       "  'publish_time': '4개월 전',\n",
       "  'url_suffix': '/watch?v=voaKr_JHvF4&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content='The AI world is back with another three-letter acronym. Last time it was MCP or the model context protocol. This time it\\'s A2A from Google and it\\'s the agent to agent protocol. Let\\'s see what it is, how it works, and does it replace or complement MCP. Let\\'s get right into [Music] it. that understand this ADA protocol. It\\'s really good to understand MCP which is a related protocol. I\\'ve got a full overview video of that and there\\'s a link in the description right down below. Now before we begin, one little thing. This is a copyrighted video and if you\\'re not watching it on the blue color coder channel, you\\'re not in the right place and I do not consent to this being rebroadcast on any other platform. So let\\'s say that you want to book a flight. So you\\'ve got Claude desktop over here. Claude Desktop is an MCP client. It can connect to an MCP server from the airline and say, \"List tools. What do you have available for me to interact with your airline?\" The airline would send back, \"I\\'ve got tools to search flights and book flights.\" Your MCB client says, \"Hey, that\\'s great. I want you to search for a flight for Portland to Honolulu.\" It sends back some JSON data encoded as text. Claude then selects the flight that I want to go on to and then invokes the bookflight tool which returns, hey, I got a flight for you. What\\'s really going on here is that Claude is interacting with an LLM or a language learning model and invoking tools within that MCP server to get that done. So that\\'s the model contracts protocol and people nowadays are calling it the USBC for AIS. A toa is this agentto agent protocol. So let\\'s start talking about what an agent is. In the ADA protocol model, an agent is a combination of an LLM or large language model and tools. The agent takes an incoming task, uses the LLM to reason about that task, and then tools to execute on that task. And the ADA protocol allows either you or an agent to talk to another agent. So let\\'s take that example scenario of you booking a flight. Now, you would have access to some AA client. Now that\\'s actually not doable today. There are no kind of consumer A to A clients. All we have right now for A to A is this Google GitHub repo that has some sample code in it. It is not actually productized in any way. So this is what\\'s going to be in the future. So you\\'ve got your A to client. It has access to an LLM. It sends off a chat style message. Please book a flight. Here are the specifics of that flight. That agent that exists within that ATA server then uses its own LLM to parse that request and use the tools that we talked about before searching flights and booking flights internally to go and book that flight. And then it sends back a chat style response. Again, agents are LLMs plus tools. And this is agents talking with other agents across that A2A protocol. Now, if that wasn\\'t cool enough, the really interesting part about ADA is that I can connect with, for example, a travel agent agent. That travel agent would have its own LLM that it\\'s connected to, but it would also be connected to downstream agents like an airline agent, a hotel agent, and a car agent. So conceivably I could make a request like hey go and book an entire trip to Honolulu and it would figure out I need to go on this airline at this time go into this hotel rent this car and all that and the way that it\\'s going to do that is that that travel agent is going to use that same AA protocol to connect itself to the airline agent hotel agent and that car agent. So here\\'s the ADA site. It\\'s a really good read and you can also get some samples and in this case that is the Google ADA repo. Now that repo not only has example agents in both JavaScript and Python but it\\'s also got a demo application that you can connect to that acts as a multi- aent kind of like the travel agent in our example. It\\'s a single site that then connects to multiple agents simultaneously to try and get something done. So, I\\'ve actually built on top of that demo code and built a hotel agent and a flight agent that we can connect to on that demo. So, those are both running in my terminal. Here we have our hotel agent and here we have our flight agent and we\\'ve got the demo app running over here in our third terminal. So, let\\'s go into our browser, see how we go. This is really beta stuff, so this might work and might not. So, this is the demo UI and it\\'s actually currently hooked up to both those agents. So, let\\'s try it out. Let\\'s ask it to book a trip to Maui. All right, this is actually a pretty good response from what I\\'ve seen. So, it\\'s telling us it\\'s going to coordinate with both the flight agent and hotel agent. I apologize in advance for the CSS here. That\\'s was part of the demo. All right, I\\'ll give it a little more information and we\\'ll see what happens. All right, it looks like it\\'s interacting with the flight agent pretty well, but it\\'s not doing that well with hotels. But I think you can see the point and also the status of the project. It\\'s a really cool idea. It\\'s just very early days. Now, there is one more way to interact with those agents that you might be interested in if you want to try this out for yourself. If I go into that samples.js directory and then I run this ADA CLI client and then I give it the URL of the agent that I want to talk to. In this case, we\\'ll go with the hotel agent. We give it that hotel agent URL. This brings up an interactive terminal with that specific agent. Now, one interesting thing right away is we have this agent card that is returned to us and that gives all the specifics about that particular agent. There\\'s more to it than what we\\'re seeing here. I\\'ll show you that in just a bit. But we can actually have a chat session with it right now. So, I can do / new to start a new task. That\\'s a core feature of the ADA protocol. You have tasks with these agents. All right. I\\'ve used this ADA client CLI tool to make the request of tell me what hotels you have available to that hotels agent. and it\\'s gotten back a reasonably decent response based on that demo data. Let\\'s take a very quick look at one of these servers themselves to give you some background on that. I\\'m a JavaScript person, so that\\'s what I used for this. We\\'ll take a look at that hotel agent. As I mentioned before, this is just demo data that I created over in hotels.ts. But the really fun stuff is over in index.ts. So to start our server, we bring in the class for ADA server from the local server index.js file. That\\'s actually a really important indicator about where this project is in terms of its deployment. There are no A to packages out in the node module space right now to support A to A. If you want to develop on this, you\\'re literally going to be using the PC code for the protocol itself. Right down the bottom here, we instantiate that A to server with one of the most important things, which is our card. The card, and all AA servers have cards, is essentially the calling card of the agent. has the name, the description, the current URL, although I think you have gotten the URL, but whatever, it\\'s okay. The version, the capabilities, whether it actually streams responses or not, and then an array of skills, and it\\'s those skills that allow a multi- aent, that\\'s an agent that talks to other agents to decide what agents to use and when. The rest of this sets up a handler, and that handler takes incoming requests and then passes them off to the AI of your choice. In this case, it uses Google\\'s Genkit and the underlying model is Gemini 20 Flash. Of course, you get to process those messages however you want and send them to any model you want. If you want to send the O Lama, Open AI, hugging face, whatever you want to do, you can do that. All right. So, given what I know today, let\\'s talk about what\\'s good and bad with ADA. So, AAA uses JSON RPC as the schema to talk between the two ADA client and server instances. And I think that\\'s a fantastic choice when it comes to overthe-wire communications. and it is what MCP uses. So, it\\'s a fantastic analog to MCP. I love the idea that they have of an agent marketplace that there would be some registry where my multi- aent could go and say, \"Hey, give me an agent that does this. It would give me back a URL. It would get the card and start talking to the agent.\" I think that\\'s a fantastic concept. I love this idea of this agent card that gives back to you either as a human or as another agent to see, is this going to actually do the kind of work that I want to do. Another great feature of ADA is it allows us to select the right model for the use case. Each one of these agents gets to decide which model it\\'s going to use to service its request. In the original example with Claude and MCP, we\\'re going to keep using that Claude sonet model for every one of our use cases and that might not be the right model. ADA allows us to say we want this model for this request and even have tuned models per agent. Authentication is built right into the protocol from the get-go. This is a huge advantage over what we got originally out of the box with MCP which didn\\'t have any authentication but now does. I also like that they\\'ve learned other lessons from MCP. Like for example, ADA does not support the standard IO protocol. You can\\'t just run one of these servers as a command. I think that\\'s actually a really good way to go because if you had that then you wouldn\\'t be able to do the registry or the agent marketplace concept that they were talking about. Also, I think the docs at this point in the evolution of the product are excellent. All right, so what\\'s not so good? Well, the current code state, I would say the actual code docs need a lot of work. So, unless you\\'re a developer that\\'s familiar with both JavaScript and Python, it\\'s going to be a while before you get this set up and running. And there\\'s really not a lot of good feedback when it comes to what\\'s wrong with your setup. So, you\\'re going to need to do a lot of debugging and digging in, which for me is fun, but I know for a lot of folks is frustrating. I think reliability and testing is going to be a problem particularly as these agent networks grow. And then there\\'s what I\\'m calling the multi- aent hairball. So what\\'s that? Well, think about this multi- aent, the travel agent. The agent that\\'s talking to other agents. Well, that agent, he\\'s going to be really complex. As you saw when you\\'re doing that interactive example in the browser, that agent is going to have to plan and connect with all these agents. it\\'s going to be a lot of work to get that multi- aent right and there\\'s going to be multiple different flavors of that multi- aent. So to me that\\'s really a hairball and in particular where the agent is connecting with other agents and making requests of other agents your initial prompt to the agent is going to be from you but from following from then on that multi- aent is going to create its own prompts and be talking to agents on its own. So being able to trace those connections and see what prompts it\\'s generated, we\\'re going to need some serious work in there. And then finally, the idea of a centralized identity or billing system. If I have this registry and I\\'m going out and talking to all those agents, do I have to have unique contracts with each one of those? That\\'s going to be something that\\'s going to be interesting to see as this standard evolves. So will AA replace MCP? Well, I don\\'t think so. MCP is a lower level standard. and it it presents tools to LLMs. We\\'re always going to need those tools at that level of granularity. And ADA doesn\\'t give us access to those tools at that level of granularity. And I know we\\'ve all had frustration with trying to get the prompts to do what we want them to do. So having MCP at that level of granularity, we can\\'t replace that with A to A. So do we need A to A if we have MCP? Maybe, maybe not. Actually, I mean, if you\\'ve got a tool and that tool invokes an LLM to get stuff done and the new version of the MCB protocol can stream responses back, yeah, you could probably do something similar to what ADA is doing right now inside of MCP. But I do think that would be a misuse of MCP. And I think that having a specific protocol for agents to talk to other agents is valuable. But at the end of the day, I think as the docs say, ADA loves MCP. LLM plus tools are agents. MCP gives agents those tools. So that\\'s why ADA and MCP play really nicely together. All right, if you have any questions or comments, be sure to put that in the comment section right down below. And in the meantime, if you like this video, hit that like button. If you really like the video, hit the subscribe button and click on that bell. You\\'ll be notified the next time a new blue collar coder comes out.')],\n",
       "  'summary': '이번 영상에서는 구글의 A2A(Agent to Agent) 프로토콜에 대해 다룹니다. A2A는 이전에 소개된 MCP(모델 컨텍스트 프로토콜)와 관련이 있으며, 두 프로토콜 간의 차이점과 상호 보완 관계를 설명합니다. \\n\\nA2A 프로토콜에서는 에이전트가 LLM(대화형 언어 모델)과 도구를 조합하여 작업을 수행합니다. 이를 통해 사용자는 자신의 요구를 다른 에이전트에게 전달하고, 이 에이전트는 요청을 처리하여 결과를 반환합니다. 예를 들어, 사용자가 여행을 예약하려고 할 경우, 요청을 여행 에이전트에게 보내고, 해당 에이전트는 항공사, 호텔, 자동차 렌트 서비스 등과 연결하여 전체 여행을 계획합니다.\\n\\n현재 A2A 프로토콜은 개발 초기 단계에 있으며, 프로토타입과 샘플 코드만 존재합니다. 사용자는 A2A 클라이언트를 사용하여 특정 에이전트와 상호작용할 수 있습니다. 그러나 아직 상용 제품으로서는 구현되지 않았습니다. \\n\\nA2A의 장점으로는 JSON RPC 통신 방식, 에이전트 카드 시스템, 다양한 모델 선택 가능성, 그리고 내장된 인증 기능 등이 있으며, MCP와의 협력 가능성이 큰 것으로 보입니다. 반면, 코드와 문서화의 부족, 안정성 문제, 복잡한 멀티 에이전트 상호작용 등 단점도 지적됩니다.\\n\\n결론적으로, A2A가 MCP를 완전히 대체하지는 않겠지만, 서로 보완적으로 작용할 수 있을 것으로 전망됩니다. A2A와 MCP를 함께 활용하면 더 강력한 시스템을 구축할 수 있을 것입니다.'},\n",
       " {'id': 'GUozMSpnmcc',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLAainynDDWkCS5D1lD9J_wiq90GfA',\n",
       "   'https://i.ytimg.com/vi/GUozMSpnmcc/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCL6gT6oBOergoFUfhxP4D1Rp7hKQ'],\n",
       "  'title': 'Google A2A Protocol Explained: Everything You Need to Know',\n",
       "  'long_desc': None,\n",
       "  'channel': 'AI LABS',\n",
       "  'duration': '8:20',\n",
       "  'views': '조회수 5,594회',\n",
       "  'publish_time': '3개월 전',\n",
       "  'url_suffix': '/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content=\"Google has introduced a new protocol called A2A which stands for agentto agent protocol. It enables communication between two agentic applications or between two agents. And the crazy part is that this protocol can connect AI agents from any framework whether it's Langchain, Crew AI, Google's ADK or even customuilt systems. A2A runs on a shared protocol built on HTTP. This protocol is honestly wild and the kind of applications and implementations we're going to see from it are going to be mind-blowing. Just like MCP, it's expected to gain a lot of traction. As more applications are developed with it, the momentum will only grow. The best part is that it doesn't replace MCP. Instead, it works alongside it and both can be used together to build powerful systems. Let's jump into the video and see how it all works. Right now, I'm going to show you an official demo from Google. This is exactly what they demonstrated. It starts with the end user which is you. The client in the demo is Google agent space but it could be any client. It begins with a single agent called the remote agent. Based on the task you give it, this agent looks for other agents to hand the task over to. This is where the A2A protocol steps in. It enables smooth communication between two AI agents. Now what exactly is an AI agent? They're just large language models with a set of tools. These tools define what the agent can do. In this new protocol, every agent has an agent card that describes its abilities. The remote agent reads the agent cards of other agents and picks the one best suited for the task. That agent can then pass the task on to another agent, creating a chain. This forms a multi- aent workflow. This is how A2A makes the process much easier. I'll show you in the GitHub repo how it works. You can connect agents from any framework and once this protocol is in place, those agents can talk to each other without friction. Now this is the demo that they have shared. The example they've provided shows the agentto agent protocol being used to hire a software engineer along with a job description. You can clearly observe how the A2A protocol functions. First the protocol is initiated which is visible in the thinking process. Then to discover different agents suitable for the task, it examines their agent cards. These are the main source for understanding the capabilities of each agent. There are several ways to explore agent cards. In this case, there's a tool registry where it finds the sourcing agent and initiates a call to it. You can see here that additional constraints have also been provided to the agent to find the best possible candidate. Once the sourcing agent completes its task, we can see it identifies five candidates for the job. If you continue watching the video, you'll notice that two weeks later, after the interviews are done, the agent is used again to gather updates and perform background checks. Based on the candidate data, the system is capable of running a background check on a single candidate. This entire process of hiring a software engineer based on a job description is automated using these agents. The most important thing is that this wasn't handled by a single agent. The A2A protocol allowed multiple agents to work together. all communicating through a single protocol. Let's clear up another concept about the agentto agent protocol and MCP. A2A is meant to work with MCP and even Google has confirmed this. To explain why, think of it like this. MCP is an LLM with tools or access to specific data. Picture it as a repair man. He has a screwdriver and the knowledge to fix cars. That's the MCP part. But this repair man also needs to talk to others. Maybe he needs to speak with clients or borrow parts from someone else. That's where the agentto aagent protocol comes in. It allows agents to communicate with each other. These agents could even be separate MCP servers acting as independent agents. They can share tools or request help when needed. The key to all of this is the agent card. It defines what each agent is capable of and helps them interact in a structured way. They've explained a clear connection between A2A and MCP. In their official documentation, it's outlined that future Agentic applications will need both of these protocols to become more capable. As an example, they use the same auto repair shop analogy. There are multiple handyman sub agents working in the shop. They need tools, but to use those tools properly, they also need extra context from customers. Those customers could be other agents. The interesting part is how MCP fits into this setup. We know that to identify an agent using the A2A protocol, it must have an agent card. These agent cards can be listed as resources. The MCP server can then provide access to them. The LLM fetches these agent cards and passes them to sub agents. Sub agents read the cards and based on the information, the LLM decides which external agent should be used. It's a clever integration and shows how both systems can work together in a flexible way. The agent card structure is clearly defined in their documentation. It includes key information about the AI agent such as the version, name, description, and its intended use. Then it lists the skills which are the core capabilities the agent can perform. It also shows the default content type the agent supports along with the parameters it accepts. Basically, it shows the kind of input the agent needs. For some agents, authentication might be required. So, specific document details would also be part of the agent card. When an LLM or another agent tries to access this agent, it first reads the agent card. Based on that, it decides whether to use the agent and how to interact with it. This makes the accuracy of the agent card critical to how the entire agentto agent system functions. Further in the documentation, they've provided sample agent cards and methods for sending tasks and receiving responses. One of the examples is a Google Maps agent. The card includes a clear description of the tasks it can perform along with the URL and provider details. It also specifies the type of authentication the agent needs. Below that, there's a format showing how a client can send a task to a remote agent to start a new job. In one simple example, the task is to tell a joke. The response comes back as a text output from a model which delivers the joke. This is one way to send a task and get a result. Other methods are also documented below. To get started, there's no need to memorize the syntax. You can feed the documentation into cursor with the at docs feature which will pick up the context and generate code accordingly. In the GitHub repo, they've included some sample agents that show how A2A agents can be implemented. One example is using Crew AI, a simple image generation agent that uses the Google Gemini API. You can install it easily. It's a basic agent that just runs on the A2A protocol. To get started, you need to clone the full GitHub repo because the commands depend on that structure. Once it's cloned, you can run the setup using a few simple commands. Just copy and paste them into your terminal. They're very straightforward. Once you run the command, it opens a command line interface for the A2A agent. The server passes it to the crew agent which uses the Gemini API to generate an image. That image is then returned to the server and finally back to the client. This is a simple implementation and it's not widely used just yet, but it will be. We saw the same thing happen with MCP. Adoption took some time, then it exploded. The same will happen here. Once people start building on top of it, AI agents will become incredibly powerful. They'll automate a huge amount of work and change how we use AI everyday. If you found this video helpful, subscribe to the channel. And if you'd like to support the content, check out the donation link in the description. Thanks for watching and I'll see you in the next video.\")],\n",
       "  'summary': '구글은 A2A(Agent-to-Agent)라는 새로운 프로토콜을 소개했습니다. 이 프로토콜은 두 개의 에이전트 애플리케이션이나 에이전트 간의 통신을 가능하게 하며, Langchain, Crew AI, 구글의 ADK와 같은 다양한 프레임워크의 AI 에이전트를 연결할 수 있습니다. A2A는 HTTP를 기반으로 한 공통 프로토콜 위에서 작동합니다. 이 프로토콜의 응용과 구현은 매우 놀라운 가능성을 제시하고 있으며, MCP와 함께 사용될 수 있습니다. \\n\\n영상에서는 구글의 공식 데모를 통해 A2A 프로토콜의 작동 방식을 보여줍니다. 데모는 사용자가 원격 에이전트를 통해 다른 에이전트에게 작업을 위임하는 과정을 보여줍니다. AI 에이전트는 도구와 기능을 가진 대규모 언어 모델로, 각 에이전트는 자신의 능력을 설명하는 에이전트 카드를 가지고 있습니다. 원격 에이전트는 다른 에이전트의 카드를 읽고 가장 적합한 에이전트를 선택하여 작업을 전달합니다. \\n\\n예시로는 소프트웨어 엔지니어를 채용하는 과정이 자동화된 모습을 보여줍니다. 여러 에이전트가 하나의 프로토콜을 통해 협력하여 작업을 수행하는 방식이 인상적입니다. A2A 프로토콜은 MCP와 함께 작동하며, 예를 들어 MCP는 특정 데이터를 처리할 수 있는 도구를 가진 대규모 언어 모델이며, A2A는 에이전트 간의 상호작용을 가능하게 합니다.\\n\\nA2A 프로토콜과 MCP는 앞으로의 에이전틱 애플리케이션에서 모두 사용될 필요가 있습니다. 에이전트 카드는 각 에이전트의 능력을 정의하고, 이를 통해 에이전트 간의 상호작용을 구조화하는 데 중요한 역할을 합니다. \\n\\n영상은 또한 A2A 에이전트를 구현하는 방법과 GitHub에서 제공하는 샘플 에이전트를 소개하며, 이러한 기술이 널리 사용될 것이라는 기대감을 표현합니다. AI 에이전트는 많은 작업을 자동화하고 우리의 일상적인 AI 사용 방식을 변화시킬 것으로 보입니다.'},\n",
       " {'id': 'IMcDEvXRBkY',\n",
       "  'thumbnails': ['https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEjCOgCEMoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLDQ5VbTviH_q8QXcX012o6KOSKl3w',\n",
       "   'https://i.ytimg.com/vi/IMcDEvXRBkY/hq720.jpg?sqp=-oaymwEXCNAFEJQDSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCoS4vMaCArJgyD0UyAn-EEHhGJ4A'],\n",
       "  'title': 'MCP or A2A',\n",
       "  'long_desc': None,\n",
       "  'channel': \"John Savill's Technical Training\",\n",
       "  'duration': '19:59',\n",
       "  'views': '조회수 15,017회',\n",
       "  'publish_time': '1개월 전',\n",
       "  'url_suffix': '/watch?v=IMcDEvXRBkY&pp=ygUMV2hhdCBpcyBBMkE_',\n",
       "  'content': [Document(metadata={'source': 'https://youtube.com/watch?v=GUozMSpnmcc&pp=ygUMV2hhdCBpcyBBMkE_'}, page_content='Hey everyone. In this video, I want to dive into do I use MCP or A2A. There\\'s been a lot of conversation about what they do and when should I use which of them. So, I want to just quickly go over well what do they do? Then it should become apparent which do I use where and actually in a lot of scenarios you will use both of them. So when we think about our AI application, so I have my AI app and if we\\'re talking about generative AI, my AI application is going to be talking to a language model. This could be a large language model, a small language model, whatever that is. that hey this has been pre-trained on lots of information and based on those probability distributions it can generate token the next one the next one next one so our application gives it some prompt and then it will generate a response but we have to remember these language models are pre-trained on a certain body a corpus of knowledge and what it can respond to is really limited to what it was trained on which is finite in size might be vast but it\\'s still finite and it has a cut off time. It was only trained on things up to a certain date and so it can only generate a response based on its training and what you include in the prompt you send to it. And so to be really useful, we have to provide that large language model with additional knowledge with information about potential other actions that it could ask the app to perform on its behalf and then send the result. And so what we do to get that additional data is we use retrieve augmented generation. Maybe it\\'s graph rag where it focuses on the relationships between things. And so I may have this database. Maybe if we have a vector database in front of it to make it easy to find things based on what is the meaning of what we\\'re asking it, but it has this whole body of knowledge and then we can go and get additional information and we can then include it in the prompt. So now the model is not only data it was trained on but we\\'re giving it additional information that will help it in the goal we\\'re given. So hey, give me a summary of this research paper. We might include information about the research paper in it as well. And then we also give that knowledge and we may also have tools. So I might think well also I have some tool over here. And then once again what I would do as part of the prompt I would describe the tool. I would have to say hey the tool is capable of doing this and you can ask me to go and run things against that tool and then I\\'ll go and send you the response. So I would have to describe this tool as part of the prompt so it would know hey I can use this go and do this for me. The problem is twofold all these different resources and tools knowledge well they have their own way of communication. So my application would have to know well this protocol and this type of information I have to speak that would also have to go and be able to speak what the tool structure is and then I have to understand it enough to not only be able to communicate with them but then potentially describe them to the large language model so it knows when to potentially ask me to hey go and ask this knowledge for this piece of information or go and call this tool and send me the response. So it gets very challenging or potentially the providers have to go and write a whole bunch of code just for my particular app or platform that I\\'m running. So there\\'s there\\'s a lot of challenges with that. Hence the model context protocol or MCP. So this is a standard way that an AI application can go and interact with various types of knowledge, various types of tools. It can even get information about hey what are some good prompt structures that you can use to work with me. Now it uses a client server architecture. So the way MCP is going to work is there\\'s a standard client and it has a onetoone mapping. So I could think about okay my app is going to have an MCP client and this is the the same my app only ever talks hey to MCP to the client then there\\'ll be an MCP server now the MCP server speaks the MCP protocol between the client and the server. But this MCP server is specific to this particular resource. So it also speaks what\\'s required to go and speak to this knowledge store. So now me as the application I don\\'t need to worry about the specifics of this at all. I just speak MCP and there\\'s a standard library available. My MCP client talks to one of the number of MCP servers using the MCP protocol and then I can use this functionality. So it\\'s both the client piece. I\\'m going to write a server piece for my solution. Well, the service provider would write the MCP server and then I can talk it and it would work on this side as well. So again, there would be an MCP client that my app talks to. So there\\'s one client per MCP server I\\'m talking to. And then again, there\\'ll be an MCP server for this particular tool. And again, this MCP server speaks this tool format. So that tool provider writes an MCP server for their solution. this database or knowledge store writes an MCP server for their solution. And what\\'s great about this is it means well for the provider they only have to write a single MCP server. They don\\'t have to go and write a ton of different servers and plugins for all the different tools. They just write an MCP server. And so what\\'s great for now me as an app, I just have to speak MCP. I don\\'t care about the details of their particular protocols. is been abstracted for me. Now these MCP servers can actually live in different places because we do also have to think about security. So it\\'s possible the MCP server is provided by that provider but it actually runs locally on my box or it could be a remote MCP server. So there\\'s different ways that MCP works. If it\\'s local, they might give me a container format so I can just run it locally and then maybe I just use standard IO to talk to it. If it\\'s remote, then I\\'m going to use TLS encryption and server sent events to that communication. A lot of them do run locally today because initially in the specification, it was missing the bit about authentication which is pretty big for security, but that has been updated. So MCP servers now um act as an OOTH resource server and so I can use standard ways to do my authentication and token issuance. I can use a proper authorization service like entry ID or octo or any oorth compliant IDP. Whole point of all of this the MCP server is abstracting the specifics of how to talk and leverage any of these services. So now, hey, it\\'s really easy for my app to go and use all of these things. My app just has to talk MCP. Now, the other nice feature of MCP is reflection. My app can get the MCP client to ask, hey, what do you do? And it might say, well, hey, I provide resources. So, structured data and documents that can be sent to large language models. Think rag. So it could be, hey, I provide resources. It could be, hey, I provide tools. So they\\'re functions that I can call via the MCP to perform something. It can even give me a set of prompts that are predefined instructions or templates so it can do particular tasks. So what\\'s really nice about this is I have to describe it to the large language model. Hey, what are the things available? I don\\'t have to work that out with the reflection. They just tell my app, hey, here\\'s what I\\'m capable of. And then I just include that in my system prompt. So now large language model will know, oh, okay, I need you to call this or I need you to call this with this set of instructions. So it\\'s a really powerful capability. And the large language model doesn\\'t have to know anything about MCP. It just sees inputs. it returns outputs. It\\'s the app that\\'s actually just talking MCP and large language models already great at being told, hey, here\\'s some knowledge. Here are some actions. So, there\\'s really not a lot I would have to do. So, this simplifies the process of my app wanting to talk to other knowledge and tools and get help with what are the right prompt formats so that it can perform a better service and give the large language model more capability. So it\\'s providing a standard simple way for my AI app to integrate with other knowledge, other tools that can then be leveraged by the large language model. I don\\'t have to write my own interfaces. So that\\'s MCP. Hey, my app wants to talk to other stuff to help make my application more powerful, give the large language model more capabilities. And nearly every app you write is typically going to involve some additional knowledge and/or some additional tools. So nearly everything will probably want to take advantage of MCP. But often my AI application, well, it may actually be an agent. So maybe my app is actually an agent. And in the agent world, maybe it\\'s the human sort of prompted asking it to go something. Maybe it\\'s autonomous. And as part of an AI agent, we remember we like to keep them very focused on a particular task. We don\\'t like agents to have very broad sets of capabilities. It tends not to do well. It might start hallucinating. It may get confused about well, which knowledge should it use? So, an agent has a distinct task that it accomplishes using the knowledge and the tools that it has available to it. But if my agent has a very specific task, most likely then my agent is going to want to work with one or more other agents to complete my task. So if this is an agent, then very very likely, well, there\\'s an agent B, there\\'s an agent C, and they each do their own particular task. And again, remember all of these are probably using MCP. So maybe this has a particular set of tooling it uses and it\\'s using MCP to do that. Remember there\\'s it\\'s got his client, the server, etc., etc. Maybe there\\'s some set of knowledge. It\\'s talking MCP to do that. This agent, hey, maybe it\\'s just a set of knowledge. It\\'s going to use MCP anytime I want to be able to hook into other knowledge other tools. Hey, MCP is the standard simple way to do that. But these agents need to work together. Maybe it\\'s a task planner. My app is a task planner and the request is, hey, summarize a document and then schedule a meeting with the key points. So my task planner agent maybe processes the request but then it has to call this one to maybe summarize the document and then this gives it the key points when it has to send it to this one to go and book a meeting and populate the agenda. So now I\\'ve got requirements and it\\'s well how do I know the capabilities of these agents so I know which one to call and then how do I talk to it? So I have two challenges A to A as agent to agent solves this. So it provides the communication capability. So it provides the protocol. So when I think of A to A, what I\\'m doing here is A to A is the way that they can communicate. It is a JSON based protocol. I think it\\'s JSON RPC 2.0. It\\'s over HTTPS. Now, there are other options. I could use a message bus if I wanted to, but primarily it\\'s going to be that JSON based RP2 over HTTPS. With A to A, one is acting kind of like a client and one is acting like a server. One is initiating the request to another agent. as part of hey I need to you to perform some task. Now how do I know as this agent what this agent\\'s capabilities are and what this agent\\'s capabilities are? They have an agent card. Think of it like a human being when you meet someone. You used to give them a business card and it would say hey here\\'s who I am. Here\\'s how you can contact me. These are the methods of communication that I support. Telephone or fax, uh, email, whatever. It might have my job title on it. Maybe a description of the services I offer. And so it\\'s the same thing here. Hey, I initiate a communication. And as part of that, agent B is going to send me its cards. This will have its name. There might be a version. There\\'s going to be skills that it supports. There\\'s going to be capabilities it has like it supports streaming, it supports push notifications, etc. And so this is my agent card. And likewise, this is going to have one with its stuff on it. And I\\'m going to have one that I\\'ll share as well. So these can understand who I am. And they\\'re going to share their cards. So now by sharing the card, my agent understands its skills and it understands well what features it sports. Can I stream to it? Can I do a push notification to it. Now I know which agent to call to do what thing. It also includes the authentication mechanisms. Yes, authentication is actually a core part of the protocol and so it\\'s going to be a very simple way to start on boarding and utilizing this and that card is just a well-known URL. So on the server there\\'s like a well-known agent card URL I can use and we use the A2A protocol to communicate with them. So I have a way to communicate and I have a way to understand what their capabilities are. So my agent knows well which agent that are available to me should I be using. Now the interaction is going to be based around a certain task. So as I think about okay well I want to work and do something it\\'s based around a task that I need to complete and then the agents will exchange messages towards that. So they\\'re going to send messages potentially back and forth to say how they\\'re going. The messages could be text, images, audio, video, structured data like JSON or anything else. They may share context where it\\'s required. And these can actually be asynchronous interactions. So it could be a very longunning task. The agents don\\'t have to stay continuously connected. Again, it depends on their capabilities. It may be multi-turn as they collaborate on the task. There is a full task life cycle management available as part of the protocol. But then once the task is completed, they\\'re going to share artifacts. So the artifact is the end result of that. So by using A2A agent to agent, you\\'re removing the barriers between agents working together because otherwise I would have to work out, well, how can you talk to me? I would now have the same challenges as kind of over here, but how do I talk to this agent? How do I authenticate to this agent? How do I know what its capabilities are so I know when to do it? This standardizes all of that. It\\'s actually possible I might discover other agents because I\\'m being sent it by something else that says, \"Hey, there are these agents out there. You can go and leverage this.\" The key point, don\\'t think about it. Is it MCP or A2A? They\\'re doing very different things. Most likely you\\'re going to leverage both of them. So I use MCP when I as my app could be an agent wants to talk to other resources, other tools. So it lets me just understand MCP. I don\\'t have to worry about the specifics of protocol to go and actually then go and talk to a knowledge go and talk to a tool or anything else and they\\'re all likely to use MCP anytime the app wants to hook into something else. I use A2A when hey I\\'m an agent or maybe I\\'m an app and I want to call some agents and I want a standard way of doing that. I want a way that they understand that\\'s a standard protocol and it has the mechanism so I can understand well what does this agent actually do so I know when to talk to it and it\\'s not just when I know when to talk to it these agent cards I can then feed in as part of the prompts the last language model language model knows to tell my agent hey go and ask this agent to go and do this thing send me the result and then that\\'s how they can then go and work together. So hopefully that all makes sense. Don\\'t think of it as which do I use. It\\'s great other tools, other resources for my app MCP. I need to go and talk to other AI agents. Hey, I\\'ll use A to A and they all work very happily and live happily ever after. I hope that helped clear it up and until next video, take care.')],\n",
       "  'summary': '이 영상에서는 MCP(모델 컨텍스트 프로토콜)와 A2A(에이전트 간 통신)에 대해 설명하고, 각각의 용도와 사용 방법에 대해 논의합니다. 영상은 AI 애플리케이션과 생성적 AI 모델을 활용하는 방법을 다루며, 이러한 모델이 어떻게 프롬프트에 따라 응답을 생성하는지를 설명합니다. 그러나 이러한 언어 모델은 제한된 지식 기반에서 훈련되었기 때문에, 추가적인 정보를 제공하기 위해 우리는 RAG(정보 검색 보강 생성) 같은 기법을 사용해야 합니다.\\n\\nMCP는 다양한 지식과 도구와의 상호작용을 표준화하는 시스템으로, 애플리케이션이 MCP 클라이언트를 통해 여러 MCP 서버와 통신할 수 있게 해줍니다. 즉, 애플리케이션 개발자는 각 도구나 데이터베이스의 구체적인 통신 방법에 대해 걱정할 필요 없이 MCP를 사용해 쉽게 상호작용할 수 있습니다. \\n\\n반면 A2A는 에이전트 간의 통신을 가능하게 하는 프로토콜로, JSON 기반의 통신 방식입니다. 각 에이전트는 자신의 능력과 지원하는 기능을 담은 \"에이전트 카드\"를 통해 서로의 정보를 공유하여, 어떤 에이전트와 통신할지를 결정하는 데 도움을 줍니다.\\n\\n결론적으로, MCP는 다른 도구와 리소스에 대한 접근을 간소화하며, A2A는 여러 에이전트 간의 효율적인 통신을 지원합니다. 두 프로토콜 모두 필요에 따라 함께 사용될 수 있으며, 대부분의 AI 애플리케이션은 이 두 가지 모두에 의존하게 될 것입니다.'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 영상 요약하기\n",
    "# 모든 비디오에 대해 요약을 생성\n",
    "from tqdm import tqdm # tqdm은 진행 상황을 보여주는 라이브러리\n",
    "\n",
    "for v in tqdm(videos):\n",
    "    v['summary'] = chain.invoke({\"context\": v['content']})\n",
    "   \n",
    "videos"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
